# -*- coding: utf-8 -*-
"""
ERP è³‡æ–™å‚™ä»½ä¸»ç¨‹å¼
æ•´åˆ Ragic API è³‡æ–™ç²å–ã€è³‡æ–™è½‰æ›ã€BigQuery ä¸Šå‚³çš„å®Œæ•´æµç¨‹
"""

import os
import logging
import datetime
from typing import Dict, Any, Optional

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from ragic_client import create_ragic_client, RagicClient
from data_transformer import create_transformer, DataTransformer
from bigquery_uploader import create_uploader, BigQueryUploader
from email_notifier import send_backup_notification


class ERPBackupManager:
    """ERP è³‡æ–™å‚™ä»½ç®¡ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å‚™ä»½ç®¡ç†å™¨

        Args:
            config: è¨­å®šå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„è¨­å®šåƒæ•¸
        """
        self.config = config
        self.ragic_client: Optional[RagicClient] = None
        self.transformer: Optional[DataTransformer] = None
        self.uploader: Optional[BigQueryUploader] = None

        # è¨­å®šæ—¥èªŒ
        self._setup_logging()

        # é©—è­‰è¨­å®š
        self._validate_config()

        logging.info("ERP å‚™ä»½ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _setup_logging(self):
        """è¨­å®šæ—¥èªŒæ ¼å¼"""
        log_level = self.config.get('log_level', 'INFO').upper()
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

        logging.basicConfig(
            level=getattr(logging, log_level),
            format=log_format,
            handlers=[
                logging.StreamHandler(),
                # å¯ä»¥åŠ å…¥æª”æ¡ˆè™•ç†å™¨
                # logging.FileHandler('erp_backup.log')
            ]
        )

    def _validate_config(self):
        """é©—è­‰è¨­å®šåƒæ•¸"""
        required_fields = [
            'ragic_api_key',
            'ragic_account',
            'ragic_sheet_id',
            'gcp_project_id',
            'bigquery_dataset',
            'bigquery_table'
        ]

        missing_fields = []
        for field in required_fields:
            if not self.config.get(field):
                missing_fields.append(field)

        if missing_fields:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„è¨­å®šæ¬„ä½: {', '.join(missing_fields)}")

        logging.info("è¨­å®šé©—è­‰å®Œæˆ")

    def initialize_clients(self):
        """åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ¶ç«¯"""
        try:
            # åˆå§‹åŒ– Ragic å®¢æˆ¶ç«¯
            self.ragic_client = create_ragic_client(
                api_key=self.config['ragic_api_key'],
                account=self.config['ragic_account'],
                timeout=self.config.get('ragic_timeout', 30),
                max_retries=self.config.get('ragic_max_retries', 3)
            )

            # åˆå§‹åŒ–è³‡æ–™è½‰æ›å™¨
            self.transformer = create_transformer()

            # åˆå§‹åŒ– BigQuery ä¸Šå‚³å™¨
            self.uploader = create_uploader(
                project_id=self.config['gcp_project_id'],
                location=self.config.get('bigquery_location', 'US')
            )

            logging.info("æ‰€æœ‰å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logging.error(f"å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def test_connections(self) -> Dict[str, bool]:
        """
        æ¸¬è©¦æ‰€æœ‰é€£ç·š

        Returns:
            Dict[str, bool]: å„æœå‹™çš„é€£ç·šç‹€æ…‹
        """
        results = {}

        # æ¸¬è©¦ Ragic é€£ç·š
        if self.ragic_client:
            results['ragic'] = self.ragic_client.test_connection()
        else:
            results['ragic'] = False

        # æ¸¬è©¦ BigQuery é€£ç·š
        if self.uploader:
            results['bigquery'] = self.uploader.test_connection()
        else:
            results['bigquery'] = False

        logging.info(f"é€£ç·šæ¸¬è©¦çµæœ: {results}")
        return results

    def get_last_sync_timestamp(self) -> int:
        """
        ç²å–æœ€å¾ŒåŒæ­¥æ™‚é–“æˆ³

        Returns:
            int: æœ€å¾ŒåŒæ­¥æ™‚é–“æˆ³ï¼ˆæ¯«ç§’ï¼‰
        """
        try:
            if self.uploader:
                return self.uploader.get_last_sync_timestamp(
                    self.config['bigquery_dataset'],
                    self.config['bigquery_table']
                )
            else:
                raise Exception("BigQuery ä¸Šå‚³å™¨æœªåˆå§‹åŒ–")

        except Exception as e:
            logging.warning(f"ç„¡æ³•ç²å–æœ€å¾ŒåŒæ­¥æ™‚é–“: {e}ï¼Œä½¿ç”¨é è¨­å€¼")
            # è¿”å›ä¸€é€±å‰çš„æ™‚é–“æˆ³
            last_week = datetime.datetime.now() - datetime.timedelta(weeks=1)
            return int(last_week.timestamp() * 1000)

    def fetch_ragic_data(self, last_timestamp: int) -> list:
        """
        å¾ Ragic ç²å–è³‡æ–™

        Args:
            last_timestamp: æœ€å¾Œæ›´æ–°æ™‚é–“æˆ³

        Returns:
            list: åŸå§‹ Ragic è³‡æ–™

        Raises:
            Exception: ç•¶è³‡æ–™ç²å–å¤±æ•—æ™‚
        """
        if not self.ragic_client:
            raise Exception("Ragic å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")

        logging.info("é–‹å§‹å¾ Ragic ç²å–è³‡æ–™...")

        try:
            data = self.ragic_client.fetch_data(
                sheet_id=self.config['ragic_sheet_id'],
                last_timestamp=last_timestamp,
                limit=self.config.get('ragic_page_size', 1000)
            )

            logging.info(f"æˆåŠŸç²å– {len(data)} ç­† Ragic è³‡æ–™")
            return data

        except Exception as e:
            logging.error(f"Ragic è³‡æ–™ç²å–å¤±æ•—: {e}")
            raise

    def transform_data(self, ragic_data: list) -> list:
        """
        è½‰æ›è³‡æ–™æ ¼å¼

        Args:
            ragic_data: åŸå§‹ Ragic è³‡æ–™

        Returns:
            list: è½‰æ›å¾Œçš„è³‡æ–™

        Raises:
            Exception: ç•¶è³‡æ–™è½‰æ›å¤±æ•—æ™‚
        """
        if not self.transformer:
            raise Exception("è³‡æ–™è½‰æ›å™¨æœªåˆå§‹åŒ–")

        logging.info("é–‹å§‹è³‡æ–™è½‰æ›...")

        try:
            transformed_data = self.transformer.transform_data(ragic_data)
            logging.info(f"æˆåŠŸè½‰æ› {len(transformed_data)} ç­†è³‡æ–™")
            return transformed_data

        except Exception as e:
            logging.error(f"è³‡æ–™è½‰æ›å¤±æ•—: {e}")
            raise

    def upload_to_bigquery(self, transformed_data: list) -> Dict[str, Any]:
        """
        ä¸Šå‚³è³‡æ–™è‡³ BigQuery

        Args:
            transformed_data: è½‰æ›å¾Œçš„è³‡æ–™

        Returns:
            Dict[str, Any]: ä¸Šå‚³çµæœ

        Raises:
            Exception: ç•¶ä¸Šå‚³å¤±æ•—æ™‚
        """
        if not self.uploader:
            raise Exception("BigQuery ä¸Šå‚³å™¨æœªåˆå§‹åŒ–")

        logging.info("é–‹å§‹ä¸Šå‚³è³‡æ–™è‡³ BigQuery...")

        try:
            # æ ¹æ“šè³‡æ–™é‡æ±ºå®šæ˜¯å¦æ‰¹æ¬¡ä¸Šå‚³
            batch_size = self.config.get('upload_batch_size', 1000)
            use_merge = self.config.get('use_merge', True)

            if len(transformed_data) > batch_size:
                from bigquery_uploader import batch_upload_data
                results = batch_upload_data(
                    self.uploader,
                    transformed_data,
                    self.config['bigquery_dataset'],
                    self.config['bigquery_table'],
                    batch_size
                )

                # å½™ç¸½çµæœ
                total_processed = sum(r.get('records_processed', 0) for r in results)
                success_batches = sum(1 for r in results if r.get('status') == 'success')

                return {
                    "status": "success" if success_batches == len(results) else "partial_success",
                    "method": "batch_upload",
                    "total_batches": len(results),
                    "success_batches": success_batches,
                    "total_records_processed": total_processed,
                    "batch_results": results
                }
            else:
                result = self.uploader.upload_data(
                    transformed_data,
                    self.config['bigquery_dataset'],
                    self.config['bigquery_table'],
                    use_merge=use_merge
                )

                logging.info(f"æˆåŠŸä¸Šå‚³ {result.get('records_processed', 0)} ç­†è³‡æ–™")
                return result

        except Exception as e:
            logging.error(f"BigQuery ä¸Šå‚³å¤±æ•—: {e}")
            raise

    def run_backup(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„å‚™ä»½æµç¨‹

        Returns:
            Dict[str, Any]: å‚™ä»½çµæœçµ±è¨ˆ

        Raises:
            Exception: ç•¶å‚™ä»½æµç¨‹å¤±æ•—æ™‚
        """
        start_time = datetime.datetime.now()
        logging.info(f"é–‹å§‹åŸ·è¡Œ ERP è³‡æ–™å‚™ä»½æµç¨‹ - {start_time}")

        try:
            # åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ¶ç«¯
            self.initialize_clients()

            # æ¸¬è©¦é€£ç·š
            connections = self.test_connections()
            if not all(connections.values()):
                failed_services = [k for k, v in connections.items() if not v]
                raise Exception(f"æœå‹™é€£ç·šå¤±æ•—: {', '.join(failed_services)}")

            # ç²å–æœ€å¾ŒåŒæ­¥æ™‚é–“
            last_timestamp = self.get_last_sync_timestamp()
            logging.info(f"æœ€å¾ŒåŒæ­¥æ™‚é–“æˆ³: {last_timestamp}")

            # æ­¥é©Ÿ 1: å¾ Ragic ç²å–è³‡æ–™
            ragic_data = self.fetch_ragic_data(last_timestamp)

            if not ragic_data:
                logging.info("æ²’æœ‰æ–°è³‡æ–™éœ€è¦åŒæ­¥")
                return {
                    "status": "no_data",
                    "start_time": start_time.isoformat(),
                    "end_time": datetime.datetime.now().isoformat(),
                    "duration_seconds": 0,
                    "records_processed": 0
                }

            # æ­¥é©Ÿ 2: è½‰æ›è³‡æ–™
            transformed_data = self.transform_data(ragic_data)

            if not transformed_data:
                logging.warning("è½‰æ›å¾Œæ²’æœ‰æœ‰æ•ˆè³‡æ–™")
                return {
                    "status": "no_valid_data",
                    "start_time": start_time.isoformat(),
                    "end_time": datetime.datetime.now().isoformat(),
                    "duration_seconds": (datetime.datetime.now() - start_time).total_seconds(),
                    "records_fetched": len(ragic_data),
                    "records_processed": 0
                }

            # æ­¥é©Ÿ 3: ä¸Šå‚³è‡³ BigQuery
            upload_result = self.upload_to_bigquery(transformed_data)

            # è¨ˆç®—ç¸½è€—æ™‚
            end_time = datetime.datetime.now()
            duration = (end_time - start_time).total_seconds()

            # æ•´åˆçµæœ
            result = {
                "status": "success",
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "records_fetched": len(ragic_data),
                "records_transformed": len(transformed_data),
                "upload_result": upload_result
            }

            logging.info(f"å‚™ä»½æµç¨‹å®Œæˆ - è€—æ™‚: {duration:.2f} ç§’")
            return result

        except Exception as e:
            end_time = datetime.datetime.now()
            duration = (end_time - start_time).total_seconds()
            error_msg = f"å‚™ä»½å¤±æ•— - è€—æ™‚: {duration:.2f} ç§’, éŒ¯èª¤: {str(e)}"
            logging.error(error_msg)

            return {
                "status": "error",
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "error": str(e)
            }

        finally:
            # æ¸…ç†è³‡æº
            self.cleanup()

            # ç™¼é€é›»å­éƒµä»¶é€šçŸ¥ï¼ˆå¦‚æœè¨­å®šäº†ï¼‰
            try:
                if self._should_send_notification():
                    self._send_email_notification(result)
            except Exception as e:
                logging.warning(f"ç™¼é€é›»å­éƒµä»¶é€šçŸ¥å¤±æ•—: {e}")

    def _should_send_notification(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²ç™¼é€é›»å­éƒµä»¶é€šçŸ¥"""
        return (
            self.config.get('notification_email') and
            self.config.get('smtp_from_email') and
            self.config.get('smtp_from_password')
        )

    def _send_email_notification(self, backup_result: Dict[str, Any]):
        """ç™¼é€é›»å­éƒµä»¶é€šçŸ¥"""
        smtp_config = {
            'from_email': self.config.get('smtp_from_email'),
            'from_password': self.config.get('smtp_from_password'),
            'smtp_server': self.config.get('smtp_server', 'smtp.gmail.com'),
            'smtp_port': self.config.get('smtp_port', 587)
        }

        success = send_backup_notification(
            project_id=self.config['gcp_project_id'],
            to_email=self.config['notification_email'],
            backup_result=backup_result,
            smtp_config=smtp_config
        )

        if success:
            logging.info(f"é›»å­éƒµä»¶é€šçŸ¥å·²ç™¼é€è‡³: {self.config['notification_email']}")
        else:
            logging.error("é›»å­éƒµä»¶é€šçŸ¥ç™¼é€å¤±æ•—")

    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        try:
            if self.ragic_client:
                self.ragic_client.close()
            if self.uploader:
                self.uploader.close()
            logging.info("è³‡æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logging.warning(f"è³‡æºæ¸…ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def load_config_from_env() -> Dict[str, Any]:
    """
    å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥è¨­å®š

    Returns:
        Dict[str, Any]: è¨­å®šå­—å…¸

    Raises:
        ValueError: ç•¶å¿…è¦çš„ç’°å¢ƒè®Šæ•¸ç¼ºå¤±æ™‚
    """
    config = {
        'ragic_api_key': os.environ.get('RAGIC_API_KEY'),
        'ragic_account': os.environ.get('RAGIC_ACCOUNT', 'your-account'),
        'ragic_sheet_id': os.environ.get('RAGIC_SHEET_ID', 'your-sheet/1'),
        'ragic_timeout': int(os.environ.get('RAGIC_TIMEOUT', 30)),
        'ragic_max_retries': int(os.environ.get('RAGIC_MAX_RETRIES', 3)),
        'ragic_page_size': int(os.environ.get('RAGIC_PAGE_SIZE', 1000)),

        'gcp_project_id': os.environ.get('GCP_PROJECT_ID', 'your-project-id'),
        'bigquery_dataset': os.environ.get('BIGQUERY_DATASET', 'your_dataset'),
        'bigquery_table': os.environ.get('BIGQUERY_TABLE', 'erp_backup'),
        'bigquery_location': os.environ.get('BIGQUERY_LOCATION', 'US'),

        'upload_batch_size': int(os.environ.get('UPLOAD_BATCH_SIZE', 1000)),
        'use_merge': os.environ.get('USE_MERGE', 'true').lower() == 'true',
        'log_level': os.environ.get('LOG_LEVEL', 'INFO'),

        # é›»å­éƒµä»¶é€šçŸ¥è¨­å®š
        'notification_email': os.environ.get('NOTIFICATION_EMAIL'),
        'smtp_from_email': os.environ.get('SMTP_FROM_EMAIL'),
        'smtp_from_password': os.environ.get('SMTP_FROM_PASSWORD'),
        'smtp_server': os.environ.get('SMTP_SERVER', 'smtp.gmail.com'),
        'smtp_port': int(os.environ.get('SMTP_PORT', 587))
    }

    return config


def main():
    """
    ä¸»ç¨‹å¼å…¥å£é»
    """
    try:
        # è¼‰å…¥è¨­å®š
        config = load_config_from_env()

        # å»ºç«‹å‚™ä»½ç®¡ç†å™¨
        backup_manager = ERPBackupManager(config)

        # åŸ·è¡Œå‚™ä»½
        result = backup_manager.run_backup()

        # è¼¸å‡ºçµæœ
        if result['status'] == 'success':
            print(f"âœ… å‚™ä»½æˆåŠŸå®Œæˆ")
            print(f"ğŸ“Š ç²å–è¨˜éŒ„: {result.get('records_fetched', 0)}")
            print(f"ğŸ”„ è½‰æ›è¨˜éŒ„: {result.get('records_transformed', 0)}")
            print(f"â±ï¸  ç¸½è€—æ™‚: {result.get('duration_seconds', 0):.2f} ç§’")
        elif result['status'] in ['no_data', 'no_valid_data']:
            print(f"â„¹ï¸  {result['status']}: æ²’æœ‰éœ€è¦åŒæ­¥çš„è³‡æ–™")
        else:
            print(f"âŒ å‚™ä»½å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            exit(1)

    except Exception as e:
        print(f"âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        logging.error(f"ä¸»ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        exit(1)


# Cloud Function å…¥å£é»
def backup_erp_data(request):
    """
    Google Cloud Function å…¥å£é»
    HTTP è§¸ç™¼å™¨å‡½æ•¸ï¼Œç”¨æ–¼å®šæ™‚æˆ–æ‰‹å‹•è§¸ç™¼å‚™ä»½

    Args:
        request: HTTP è«‹æ±‚ç‰©ä»¶

    Returns:
        Tuple: (å›æ‡‰å…§å®¹, HTTP ç‹€æ…‹ç¢¼)
    """
    try:
        # è¼‰å…¥è¨­å®š
        config = load_config_from_env()

        # å»ºç«‹å‚™ä»½ç®¡ç†å™¨
        backup_manager = ERPBackupManager(config)

        # åŸ·è¡Œå‚™ä»½
        result = backup_manager.run_backup()

        if result['status'] == 'success':
            return {
                "status": "success",
                "message": "ERP è³‡æ–™å‚™ä»½å®Œæˆ",
                "data": {
                    "records_processed": result.get('records_transformed', 0),
                    "duration_seconds": result.get('duration_seconds', 0)
                }
            }, 200
        elif result['status'] in ['no_data', 'no_valid_data']:
            return {
                "status": "info",
                "message": "æ²’æœ‰æ–°è³‡æ–™éœ€è¦åŒæ­¥",
                "data": result
            }, 200
        else:
            return {
                "status": "error",
                "message": result.get('error', 'å‚™ä»½å¤±æ•—'),
                "data": result
            }, 500

    except Exception as e:
        logging.error(f"Cloud Function åŸ·è¡Œå¤±æ•—: {e}")
        return {
            "status": "error",
            "message": str(e)
        }, 500


# åŸ·è¡Œä¸»ç¨‹å¼ï¼ˆæœ¬åœ°æ¸¬è©¦ä½¿ç”¨ï¼‰
if __name__ == "__main__":
    main()