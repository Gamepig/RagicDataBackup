# -*- coding: utf-8 -*-
"""
ERP è³‡æ–™å‚™ä»½ä¸»ç¨‹å¼
æ•´åˆ Ragic API è³‡æ–™ç²å–ã€è³‡æ–™è½‰æ›ã€BigQuery ä¸Šå‚³çš„å®Œæ•´æµç¨‹
"""

import os
import json
import logging
import datetime
from typing import Dict, Any, Optional, List

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from ragic_client import RagicClient
from data_transformer import create_transformer, DataTransformer
from bigquery_uploader import create_uploader, BigQueryUploader
from email_notifier import send_backup_notification


class ERPBackupManager:
    """ERP è³‡æ–™å‚™ä»½ç®¡ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å‚™ä»½ç®¡ç†å™¨

        Args:
            config: è¨­å®šå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„è¨­å®šåƒæ•¸
        """
        self.config = config
        self.ragic_client: Optional[RagicClient] = None
        self.transformer: Optional[DataTransformer] = None
        self.uploader: Optional[BigQueryUploader] = None

        # è¨­å®šæ—¥èªŒ
        self._setup_logging()

        # é©—è­‰è¨­å®š
        self._validate_config()

        logging.info("ERP å‚™ä»½ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _setup_logging(self):
        """è¨­å®šæ—¥èªŒæ ¼å¼"""
        log_level = self.config.get('log_level', 'INFO').upper()
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

        logging.basicConfig(
            level=getattr(logging, log_level),
            format=log_format,
            handlers=[
                logging.StreamHandler(),
                # å¯ä»¥åŠ å…¥æª”æ¡ˆè™•ç†å™¨
                # logging.FileHandler('erp_backup.log')
            ]
        )

    def _validate_config(self):
        """é©—è­‰è¨­å®šåƒæ•¸"""
        required_fields = [
            'ragic_api_key',
            'ragic_account',
            'ragic_sheet_id',
            'gcp_project_id',
            'bigquery_dataset',
            'bigquery_table'
        ]

        missing_fields = []
        for field in required_fields:
            if not self.config.get(field):
                missing_fields.append(field)

        if missing_fields:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„è¨­å®šæ¬„ä½: {', '.join(missing_fields)}")

        logging.info("è¨­å®šé©—è­‰å®Œæˆ")

    def initialize_clients(self):
        """åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ¶ç«¯"""
        try:
            # åˆå§‹åŒ– Ragic å®¢æˆ¶ç«¯
            self.ragic_client = RagicClient(
                api_key=self.config['ragic_api_key'],
                account=self.config['ragic_account'],
                timeout=self.config.get('ragic_timeout', 30),
                max_retries=self.config.get('ragic_max_retries', 3)
            )

            # åˆå§‹åŒ–è³‡æ–™è½‰æ›å™¨
            self.transformer = create_transformer()

            # åˆå§‹åŒ– BigQuery ä¸Šå‚³å™¨
            self.uploader = create_uploader(
                project_id=self.config['gcp_project_id'],
                location=self.config.get('bigquery_location', 'US')
            )

            logging.info("æ‰€æœ‰å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logging.error(f"å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def test_connections(self) -> Dict[str, bool]:
        """
        æ¸¬è©¦æ‰€æœ‰é€£ç·š

        Returns:
            Dict[str, bool]: å„æœå‹™çš„é€£ç·šç‹€æ…‹
        """
        results = {}

        # æ¸¬è©¦ Ragic é€£ç·š
        if self.ragic_client:
            results['ragic'] = self.ragic_client.test_connection()
        else:
            results['ragic'] = False

        # æ¸¬è©¦ BigQuery é€£ç·š
        if self.uploader:
            results['bigquery'] = self.uploader.test_connection()
        else:
            results['bigquery'] = False

        logging.info(f"é€£ç·šæ¸¬è©¦çµæœ: {results}")
        return results

    def get_last_sync_timestamp(self) -> str:
        """
        ç²å–æœ€å¾ŒåŒæ­¥æ™‚é–“

        Returns:
            str: Ragic æ ¼å¼çš„æ—¥æœŸæ™‚é–“ (yyyy/MM/dd HH:mm:ss)
        """
        try:
            if self.uploader:
                return self.uploader.get_last_sync_timestamp(
                    self.config['bigquery_dataset'],
                    self.config['bigquery_table']
                )
            else:
                raise Exception("BigQuery ä¸Šå‚³å™¨æœªåˆå§‹åŒ–")

        except Exception as e:
            logging.warning(f"ç„¡æ³•ç²å–æœ€å¾ŒåŒæ­¥æ™‚é–“: {e}ï¼Œä½¿ç”¨é è¨­å€¼")
            # è¿”å›ä¸€é€±å‰çš„æ™‚é–“ï¼ˆRagic æ ¼å¼ï¼‰
            last_week = datetime.datetime.now() - datetime.timedelta(weeks=1)
            return last_week.strftime('%Y/%m/%d %H:%M:%S')

    def fetch_ragic_data(self, last_sync_time: str) -> list:
        """
        å¾ Ragic ç²å–è³‡æ–™

        Args:
            last_sync_time: æœ€å¾ŒåŒæ­¥æ™‚é–“ (Ragic æ ¼å¼: yyyy/MM/dd HH:mm:ss)

        Returns:
            list: åŸå§‹ Ragic è³‡æ–™

        Raises:
            Exception: ç•¶è³‡æ–™ç²å–å¤±æ•—æ™‚
        """
        if not self.ragic_client:
            raise Exception("Ragic å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")

        logging.info("é–‹å§‹å¾ Ragic ç²å–è³‡æ–™...")

        try:
            # é è¨­æ¡ç”¨ï¼šå–®é æŠ“å– + æœ¬åœ°éæ¿¾ä¸€é€±ï¼ˆå¿…è¦æ™‚è‡ªå‹•ç¿»é ï¼‰
            # 1) è§£æ last_sync_timeï¼ˆRagic æ ¼å¼ï¼‰ç‚º datetime
            since_dt = None
            for fmt in ['%Y/%m/%d %H:%M:%S', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d', '%Y-%m-%d']:
                try:
                    since_dt = datetime.datetime.strptime(last_sync_time, fmt)
                    break
                except Exception:
                    continue
            if since_dt is None:
                # ç„¡æ³•è§£ææ™‚ï¼Œé€€å›ä¸€é€±å‰
                since_dt = datetime.datetime.utcnow() - datetime.timedelta(days=7)

            # 2) æ±ºå®šæœ€å¾Œä¿®æ”¹æ¬„ä½åç¨±æ¸…å–®ï¼ˆå¯ç”±ç’°å¢ƒè®Šæ•¸æä¾›ï¼Œå¦å‰‡ä½¿ç”¨é è¨­ï¼‰
            names_env = self.config.get('last_modified_field_names')
            if names_env:
                last_modified_field_names = [n.strip() for n in names_env.split(',') if n.strip()]
            else:
                last_modified_field_names = ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“']

            # 3) å¤§è¡¨æé«˜å–®é  limit
            sid = self.config['ragic_sheet_id']
            default_limit = self.config.get('ragic_page_size', 1000)
            per_sheet_boost = {'forms8/17': 3000, 'forms8/2': 3000, 'forms8/3': 3000}
            limit = per_sheet_boost.get(sid, default_limit)
            max_pages = int(self.config.get('ragic_max_pages', 50))

            data = self.ragic_client.fetch_since_local_paged(
                sheet_id=sid,
                since_dt=since_dt,
                last_modified_field_names=last_modified_field_names,
                limit=limit,
                max_pages=max_pages
            )

            logging.info(f"æˆåŠŸç²å– {len(data)} ç­† Ragic è³‡æ–™ï¼ˆlocal paged incrementalï¼‰")
            return data

        except Exception as e:
            logging.error(f"Ragic è³‡æ–™ç²å–å¤±æ•—: {e}")
            raise

    def fetch_ragic_data_for_sheet(self, sheet_id: str, last_sync_time: str, last_modified_names: Optional[List[str]] = None, limit: Optional[int] = None, max_pages: Optional[int] = None) -> list:
        """é‡å°æŒ‡å®š sheet ä»¥ã€Œå–®é æŠ“å– + æœ¬åœ°éæ¿¾ä¸€é€±ï¼ˆå¿…è¦æ™‚è‡ªå‹•ç¿»é ï¼‰ã€å–å¾—è³‡æ–™ã€‚"""
        if not self.ragic_client:
            raise Exception("Ragic å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")

        # è§£æ last_sync_time â†’ datetime
        since_dt = None
        for fmt in ['%Y/%m/%d %H:%M:%S', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d', '%Y-%m-%d']:
            try:
                since_dt = datetime.datetime.strptime(last_sync_time, fmt)
                break
            except Exception:
                continue
        if since_dt is None:
            since_dt = datetime.datetime.utcnow() - datetime.timedelta(days=7)

        # æ¬„ä½åç¨±æ¸…å–®
        if last_modified_names and isinstance(last_modified_names, list) and last_modified_names:
            names = last_modified_names
        else:
            names_env = self.config.get('last_modified_field_names')
            names = [n.strip() for n in names_env.split(',')] if names_env else ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“']

        # å–®é é™åˆ¶èˆ‡æœ€å¤§é æ•¸
        default_limit = self.config.get('ragic_page_size', 1000)
        per_sheet_boost = {'forms8/17': 3000, 'forms8/2': 3000, 'forms8/3': 3000}
        lim = limit if isinstance(limit, int) and limit > 0 else per_sheet_boost.get(sheet_id, default_limit)
        maxp = int(max_pages) if isinstance(max_pages, int) and max_pages else int(self.config.get('ragic_max_pages', 50))

        data = self.ragic_client.fetch_since_local_paged(
            sheet_id=sheet_id,
            since_dt=since_dt,
            last_modified_field_names=names,
            limit=lim,
            max_pages=maxp
        )
        logging.info(f"[{sheet_id}] å–å¾— {len(data)} ç­†ï¼ˆlocal paged incrementalï¼‰")
        return data

    def transform_data(self, ragic_data: list) -> list:
        """
        è½‰æ›è³‡æ–™æ ¼å¼

        Args:
            ragic_data: åŸå§‹ Ragic è³‡æ–™

        Returns:
            list: è½‰æ›å¾Œçš„è³‡æ–™

        Raises:
            Exception: ç•¶è³‡æ–™è½‰æ›å¤±æ•—æ™‚
        """
        if not self.transformer:
            raise Exception("è³‡æ–™è½‰æ›å™¨æœªåˆå§‹åŒ–")

        logging.info("é–‹å§‹è³‡æ–™è½‰æ›...")

        try:
            transformed_data = self.transformer.transform_data(ragic_data)
            logging.info(f"æˆåŠŸè½‰æ› {len(transformed_data)} ç­†è³‡æ–™")
            return transformed_data

        except Exception as e:
            logging.error(f"è³‡æ–™è½‰æ›å¤±æ•—: {e}")
            raise

    def upload_to_bigquery(self, transformed_data: list) -> Dict[str, Any]:
        """
        ä¸Šå‚³è³‡æ–™è‡³ BigQuery

        Args:
            transformed_data: è½‰æ›å¾Œçš„è³‡æ–™

        Returns:
            Dict[str, Any]: ä¸Šå‚³çµæœ

        Raises:
            Exception: ç•¶ä¸Šå‚³å¤±æ•—æ™‚
        """
        if not self.uploader:
            raise Exception("BigQuery ä¸Šå‚³å™¨æœªåˆå§‹åŒ–")

        logging.info("é–‹å§‹ä¸Šå‚³è³‡æ–™è‡³ BigQuery...")

        try:
            # æ ¹æ“šè³‡æ–™é‡æ±ºå®šæ˜¯å¦æ‰¹æ¬¡ä¸Šå‚³èˆ‡ä¸Šå‚³æ¨¡å¼
            batch_size = self.config.get('upload_batch_size', 1000)
            use_merge = self.config.get('use_merge', True)
            upload_mode = self.config.get('upload_mode', 'auto')
            batch_threshold = self.config.get('batch_threshold', 5000)
            staging_table = self.config.get('staging_table')
            merge_sp_name = self.config.get('merge_sp_name')

            if len(transformed_data) > batch_size:
                from bigquery_uploader import batch_upload_data
                results = batch_upload_data(
                    self.uploader,
                    transformed_data,
                    self.config['bigquery_dataset'],
                    self.config['bigquery_table'],
                    batch_size
                )

                # å½™ç¸½çµæœ
                total_processed = sum(r.get('records_processed', 0) for r in results)
                success_batches = sum(1 for r in results if r.get('status') == 'success')

                return {
                    "status": "success" if success_batches == len(results) else "partial_success",
                    "method": "batch_upload",
                    "total_batches": len(results),
                    "success_batches": success_batches,
                    "total_records_processed": total_processed,
                    "batch_results": results
                }
            else:
                result = self.uploader.upload_data(
                    transformed_data,
                    self.config['bigquery_dataset'],
                    self.config['bigquery_table'],
                    use_merge=use_merge,
                    upload_mode=upload_mode,
                    batch_threshold=batch_threshold,
                    staging_table=staging_table,
                    merge_sp_name=merge_sp_name
                )

                logging.info(f"æˆåŠŸä¸Šå‚³ {result.get('records_processed', 0)} ç­†è³‡æ–™")
                return result

        except Exception as e:
            logging.error(f"BigQuery ä¸Šå‚³å¤±æ•—: {e}")
            raise

    def run_backup(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„å‚™ä»½æµç¨‹

        Returns:
            Dict[str, Any]: å‚™ä»½çµæœçµ±è¨ˆ

        Raises:
            Exception: ç•¶å‚™ä»½æµç¨‹å¤±æ•—æ™‚
        """
        start_time = datetime.datetime.now()
        logging.info(f"é–‹å§‹åŸ·è¡Œ ERP è³‡æ–™å‚™ä»½æµç¨‹ - {start_time}")

        # åˆå§‹åŒ– result è®Šæ•¸ï¼Œé¿å… finally å€å¡Šä¸­æ‰¾ä¸åˆ°è®Šæ•¸
        result = {
            "status": "error",
            "start_time": start_time.isoformat(),
            "error": "æœªçŸ¥éŒ¯èª¤"
        }

        try:
            # åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ¶ç«¯
            self.initialize_clients()

            # æ¸¬è©¦é€£ç·š
            connections = self.test_connections()
            if not all(connections.values()):
                failed_services = [k for k, v in connections.items() if not v]
                raise Exception(f"æœå‹™é€£ç·šå¤±æ•—: {', '.join(failed_services)}")

            # ç²å–æœ€å¾ŒåŒæ­¥æ™‚é–“
            last_sync_time = self.get_last_sync_timestamp()
            logging.info(f"æœ€å¾ŒåŒæ­¥æ™‚é–“: {last_sync_time}")

            # å–®è¡¨æ¨¡å¼ï¼šå¾ Ragic ç²å–è³‡æ–™
            ragic_data = self.fetch_ragic_data(last_sync_time)

            if not ragic_data:
                logging.info("Ragic ç„¡æ–°è³‡æ–™éœ€è¦å‚™ä»½")
                end_time = datetime.datetime.now()
                return {
                    "status": "no_data",
                    "message": "Ragic ç„¡æ–°è³‡æ–™å¯ä»¥å‚™ä»½",
                    "start_time": start_time.isoformat(),
                    "end_time": end_time.isoformat(),
                    "duration_seconds": (end_time - start_time).total_seconds(),
                    "last_sync_time": last_sync_time,
                    "records_processed": 0
                }

            # æ­¥é©Ÿ 2: è½‰æ›è³‡æ–™
            transformed_data = self.transform_data(ragic_data)

            if not transformed_data:
                logging.warning("è½‰æ›å¾Œæ²’æœ‰æœ‰æ•ˆè³‡æ–™")
                return {
                    "status": "no_valid_data",
                    "start_time": start_time.isoformat(),
                    "end_time": datetime.datetime.now().isoformat(),
                    "duration_seconds": (datetime.datetime.now() - start_time).total_seconds(),
                    "records_fetched": len(ragic_data),
                    "records_processed": 0
                }

            # æ­¥é©Ÿ 3: ä¸Šå‚³è‡³ BigQuery
            upload_result = self.upload_to_bigquery(transformed_data)

            # è¨ˆç®—ç¸½è€—æ™‚
            end_time = datetime.datetime.now()
            duration = (end_time - start_time).total_seconds()

            # æ•´åˆçµæœï¼ˆå–®è¡¨ï¼‰
            result = {
                "status": "success",
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "records_fetched": len(ragic_data),
                "records_transformed": len(transformed_data),
                "records_processed": upload_result.get('records_processed', len(transformed_data)),
                "invalid_records": len(self.transformer.get_invalid_records()) if hasattr(self.transformer, 'get_invalid_records') else 0,
                "details": [
                    {
                        "sheet_code": self.config.get('sheet_code') or "",
                        "uploaded": upload_result.get('records_processed', len(transformed_data)),
                        "invalid": len(self.transformer.get_invalid_records()) if hasattr(self.transformer, 'get_invalid_records') else 0
                    }
                ],
                "upload_result": upload_result
            }

            logging.info(f"å‚™ä»½æµç¨‹å®Œæˆ - è€—æ™‚: {duration:.2f} ç§’")
            return result

        except Exception as e:
            end_time = datetime.datetime.now()
            duration = (end_time - start_time).total_seconds()
            error_msg = f"å‚™ä»½å¤±æ•— - è€—æ™‚: {duration:.2f} ç§’, éŒ¯èª¤: {str(e)}"
            logging.error(error_msg)

            # å°‡éŒ¯èª¤åŸå› ä»¥çµæ§‹åŒ–å½¢å¼å¯«å…¥æ—¥èªŒï¼Œä¾¿æ–¼ Cloud Logging éæ¿¾
            try:
                logging.error({
                    "type": "backup_error",
                    "sheet_id": self.config.get('ragic_sheet_id'),
                    "dataset": self.config.get('bigquery_dataset'),
                    "table": self.config.get('bigquery_table'),
                    "error": str(e),
                    "duration_seconds": duration,
                    "start_time": start_time.isoformat(),
                })
            except Exception:
                pass

            return {
                "status": "error",
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "error": str(e)
            }

        finally:
            # æ¸…ç†è³‡æº
            self.cleanup()

            # ç™¼é€é›»å­éƒµä»¶é€šçŸ¥ï¼ˆå¦‚æœè¨­å®šäº†ï¼‰
            try:
                if self._should_send_notification():
                    self._send_email_notification(result)
            except Exception as e:
                logging.warning(f"ç™¼é€é›»å­éƒµä»¶é€šçŸ¥å¤±æ•—: {e}")

    def _should_send_notification(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²ç™¼é€é›»å­éƒµä»¶é€šçŸ¥"""
        if os.environ.get('DISABLE_EMAIL', 'false').lower() == 'true':
            return False
        return (
            self.config.get('notification_email') and
            self.config.get('smtp_from_email') and
            self.config.get('smtp_from_password')
        )

    def _send_email_notification(self, backup_result: Dict[str, Any]):
        """ç™¼é€é›»å­éƒµä»¶é€šçŸ¥"""
        smtp_config = {
            'from_email': self.config.get('smtp_from_email'),
            'from_password': self.config.get('smtp_from_password'),
            'smtp_server': self.config.get('smtp_server', 'smtp.gmail.com'),
            'smtp_port': self.config.get('smtp_port', 587)
        }

        success = send_backup_notification(
            project_id=self.config['gcp_project_id'],
            to_email=self.config['notification_email'],
            backup_result=backup_result,
            smtp_config=smtp_config
        )

        if success:
            logging.info(f"é›»å­éƒµä»¶é€šçŸ¥å·²ç™¼é€è‡³: {self.config['notification_email']}")
        else:
            logging.error("é›»å­éƒµä»¶é€šçŸ¥ç™¼é€å¤±æ•—")

    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        try:
            if self.ragic_client:
                self.ragic_client.close()
            if self.uploader:
                self.uploader.close()
            logging.info("è³‡æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logging.warning(f"è³‡æºæ¸…ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    # ---- sheet å°ç…§ ----
    def _get_sheet_map(self) -> Dict[str, str]:
        """å–å¾— sheet_code â†’ sheet_id å°ç…§è¡¨ï¼Œå„ªå…ˆä½¿ç”¨ç’°å¢ƒè¨­å®šï¼Œå…¶æ¬¡å…§å»ºé è¨­ã€‚"""
        # å…§å»ºé è¨­
        default_map = {
            '10': 'forms8/5',
            '20': 'forms8/4',
            '30': 'forms8/7',
            '40': 'forms8/1',
            '41': 'forms8/6',
            '50': 'forms8/17',
            '60': 'forms8/2',
            '70': 'forms8/9',
            '99': 'forms8/3',
        }
        # 1) è¨­å®šä¸­å·²æœ‰è§£æå®Œæˆçš„ dict
        m = self.config.get('sheet_map')
        if isinstance(m, dict) and m:
            return m
        # 2) JSON å­—ä¸²
        mjson = self.config.get('sheet_map_json')
        if mjson:
            try:
                parsed = json.loads(mjson)
                if isinstance(parsed, dict) and parsed:
                    return parsed
            except Exception as e:
                logging.warning(f"SHEET_MAP_JSON è§£æå¤±æ•—ï¼Œä½¿ç”¨é è¨­å°ç…§: {e}")
        # 3) æª”æ¡ˆ
        mfile = self.config.get('sheet_map_file')
        if mfile and os.path.exists(mfile):
            try:
                with open(mfile, 'r', encoding='utf-8') as f:
                    parsed = json.load(f)
                    if isinstance(parsed, dict) and parsed:
                        return parsed
            except Exception as e:
                logging.warning(f"SHEET_MAP_FILE è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­å°ç…§: {e}")
        return default_map

    def run_backup_all_sheets(self) -> Dict[str, Any]:
        """å¤šè¡¨æµç¨‹ï¼šä¾å›ºå®š 9 å¼µè¡¨ï¼ˆæˆ–ç’°å¢ƒè®Šæ•¸æä¾›ï¼‰é€²è¡Œä¸€é€±å¢é‡æŠ“å–ã€è½‰æ›ã€ä¸Šå‚³ä¸¦å½™ç¸½ã€‚"""
        start_time = datetime.datetime.now()
        logging.info(f"é–‹å§‹åŸ·è¡Œ ERP å¤šè¡¨å‚™ä»½æµç¨‹ - {start_time}")

        result = {
            "status": "error",
            "start_time": start_time.isoformat(),
            "error": "æœªçŸ¥éŒ¯èª¤"
        }

        # 9 å¼µè¡¨å°ç…§ï¼ˆå¯ç”±å¤–éƒ¨è¨­å®šè¦†è“‹ï¼‰
        sheets = self._get_sheet_map()

        per_sheet_names = {
            'forms8/5': ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
            'forms8/4': ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
            'forms8/7': ['æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
            'forms8/1': ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
            'forms8/6': ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
            'forms8/17': ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
            'forms8/2': ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
            'forms8/9': ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
            'forms8/3': ['æœ€å¾Œä¿®æ”¹æ—¥æœŸ', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'æ›´æ–°æ™‚é–“', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
        }

        # é€£ç·šèˆ‡æœ€å¾ŒåŒæ­¥æ™‚é–“
        try:
            self.initialize_clients()
            connections = self.test_connections()
            if not all(connections.values()):
                failed_services = [k for k, v in connections.items() if not v]
                raise Exception(f"æœå‹™é€£ç·šå¤±æ•—: {', '.join(failed_services)}")

            last_sync_time = self.get_last_sync_timestamp()
            logging.info(f"æœ€å¾ŒåŒæ­¥æ™‚é–“: {last_sync_time}")

            total_uploaded = 0
            total_invalid = 0
            details: List[Dict[str, Any]] = []

            # é€è¡¨è™•ç†
            for sheet_code, sheet_id in sheets.items():
                try:
                    records = self.fetch_ragic_data_for_sheet(
                        sheet_id=sheet_id,
                        last_sync_time=last_sync_time,
                        last_modified_names=per_sheet_names.get(sheet_id),
                        limit=None,
                        max_pages=None
                    )

                    transformer = create_transformer(sheet_code=sheet_code, project_id=self.config['gcp_project_id'], use_dynamic_mapping=False)
                    transformed = transformer.transform_data(records)

                    uploaded = 0
                    invalid = len(transformer.get_invalid_records()) if hasattr(transformer, 'get_invalid_records') else 0
                    if transformed:
                        up_res = self.uploader.upload_data(
                            data=transformed,
                            dataset_id=self.config['bigquery_dataset'],
                            table_id=self.config['bigquery_table'],
                            use_merge=self.config.get('use_merge', False),
                            upload_mode=self.config.get('upload_mode', 'direct')
                        )
                        uploaded = up_res.get('records_processed', len(transformed))

                    total_uploaded += uploaded
                    total_invalid += invalid
                    details.append({
                        'sheet_code': sheet_code,
                        'uploaded': uploaded,
                        'invalid': invalid
                    })

                except Exception as se:
                    logging.error(f"è™•ç†è¡¨ {sheet_code} å¤±æ•—: {se}")
                    details.append({
                        'sheet_code': sheet_code,
                        'uploaded': 0,
                        'invalid': 0,
                        'error': str(se)
                    })

            end_time = datetime.datetime.now()
            duration = (end_time - start_time).total_seconds()

            status = 'success' if total_uploaded > 0 else 'no_data'
            result = {
                'status': status,
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration,
                'records_processed': total_uploaded,
                'invalid_records': total_invalid,
                'last_sync_time': last_sync_time,
                'details': details
            }
            logging.info(f"å¤šè¡¨å‚™ä»½å®Œæˆï¼Œå…±ä¸Šå‚³ {total_uploaded} ç­†ï¼Œç„¡æ•ˆ {total_invalid} ç­†")
            return result

        except Exception as e:
            end_time = datetime.datetime.now()
            duration = (end_time - start_time).total_seconds()
            logging.error(f"å¤šè¡¨å‚™ä»½å¤±æ•—: {e}")
            return {
                'status': 'error',
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration,
                'error': str(e)
            }
        finally:
            self.cleanup()
            try:
                if self._should_send_notification():
                    self._send_email_notification(result)
            except Exception as e:
                logging.warning(f"ç™¼é€é›»å­éƒµä»¶é€šçŸ¥å¤±æ•—: {e}")


def load_config_from_env() -> Dict[str, Any]:
    """
    å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥è¨­å®š

    Returns:
        Dict[str, Any]: è¨­å®šå­—å…¸

    Raises:
        ValueError: ç•¶å¿…è¦çš„ç’°å¢ƒè®Šæ•¸ç¼ºå¤±æ™‚
    """
    config = {
        'ragic_api_key': os.environ.get('RAGIC_API_KEY'),
        'ragic_account': os.environ.get('RAGIC_ACCOUNT', 'your-account'),
        'ragic_sheet_id': os.environ.get('RAGIC_SHEET_ID', 'your-sheet/1'),
        'ragic_timeout': int(os.environ.get('RAGIC_TIMEOUT', 30)),
        'ragic_max_retries': int(os.environ.get('RAGIC_MAX_RETRIES', 3)),
        'ragic_page_size': int(os.environ.get('RAGIC_PAGE_SIZE', 1000)),
        'ragic_max_pages': int(os.environ.get('RAGIC_MAX_PAGES', 50)),
        'last_modified_field_names': os.environ.get('LAST_MODIFIED_FIELD_NAMES'),
        # sheet å°ç…§è¨­å®š
        'sheet_map_json': os.environ.get('SHEET_MAP_JSON'),
        'sheet_map_file': os.environ.get('SHEET_MAP_FILE'),

        'gcp_project_id': os.environ.get('GCP_PROJECT_ID', 'your-project-id'),
        'bigquery_dataset': os.environ.get('BIGQUERY_DATASET', 'your_dataset'),
        'bigquery_table': os.environ.get('BIGQUERY_TABLE', 'erp_backup'),
        'bigquery_location': os.environ.get('BIGQUERY_LOCATION', 'US'),

        'upload_batch_size': int(os.environ.get('UPLOAD_BATCH_SIZE', 1000)),
        'use_merge': os.environ.get('USE_MERGE', 'true').lower() == 'true',
        'upload_mode': os.environ.get('UPLOAD_MODE', 'auto'),
        'batch_threshold': int(os.environ.get('BATCH_THRESHOLD', 5000)),
        'staging_table': os.environ.get('STAGING_TABLE'),
        'merge_sp_name': os.environ.get('MERGE_SP_NAME'),
        'log_level': os.environ.get('LOG_LEVEL', 'INFO'),

        # é›»å­éƒµä»¶é€šçŸ¥è¨­å®š
        'notification_email': os.environ.get('NOTIFICATION_EMAIL'),
        'smtp_from_email': os.environ.get('SMTP_FROM_EMAIL'),
        'smtp_from_password': os.environ.get('SMTP_FROM_PASSWORD'),
        'smtp_server': os.environ.get('SMTP_SERVER', 'smtp.gmail.com'),
        'smtp_port': int(os.environ.get('SMTP_PORT', 587))
    }

    return config


def main():
    """
    ä¸»ç¨‹å¼å…¥å£é»
    """
    try:
        # è¼‰å…¥è¨­å®š
        config = load_config_from_env()

        # å»ºç«‹å‚™ä»½ç®¡ç†å™¨
        backup_manager = ERPBackupManager(config)

        # è‹¥ RAGIC_SHEET_ID è¨­ç‚º ALLï¼Œèµ°å¤šè¡¨æµç¨‹ï¼›å¦å‰‡èµ°å–®è¡¨
        if (config.get('ragic_sheet_id') or '').upper() == 'ALL':
            result = backup_manager.run_backup_all_sheets()
        else:
            # å…è¨±ç‚ºå–®è¡¨æŒ‡å®š sheet_code ä»¥åˆ© email details é¡¯ç¤º
            try:
                config['sheet_code'] = os.environ.get('RAGIC_SHEET_CODE')
                # è‹¥æä¾› sheet_code ä¸”æœªæä¾›æ˜ç¢º sheet_idï¼Œå˜—è©¦ç”¨å°ç…§è¡¨æ˜ å°„
                sc = config.get('sheet_code')
                sid = config.get('ragic_sheet_id')
                if sc and (not sid or sid == 'AUTO' or sid == 'your-sheet/1'):
                    # è‡¨æ™‚å»ºç«‹ç®¡ç†å™¨ä»¥è®€å–å°ç…§
                    sm = backup_manager._get_sheet_map()
                    mapped = sm.get(sc)
                    if mapped:
                        config['ragic_sheet_id'] = mapped
                        # åŒæ­¥åˆ°å¯¦ä¾‹è¨­å®š
                        backup_manager.config['ragic_sheet_id'] = mapped
                        logging.info(f"ä»¥å°ç…§è¡¨å°‡ sheet_code={sc} æ˜ å°„ç‚º sheet_id={mapped}")
            except Exception:
                pass
            result = backup_manager.run_backup()

        # è¼¸å‡ºçµæœ
        if result['status'] == 'success':
            print(f"âœ… å‚™ä»½æˆåŠŸå®Œæˆ")
            print(f"ğŸ“Š ç²å–è¨˜éŒ„: {result.get('records_fetched', 0)}")
            print(f"ğŸ”„ è½‰æ›è¨˜éŒ„: {result.get('records_transformed', 0)}")
            print(f"â±ï¸  ç¸½è€—æ™‚: {result.get('duration_seconds', 0):.2f} ç§’")
        elif result['status'] in ['no_data', 'no_valid_data']:
            print(f"â„¹ï¸  {result['status']}: æ²’æœ‰éœ€è¦åŒæ­¥çš„è³‡æ–™")
        else:
            print(f"âŒ å‚™ä»½å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            exit(1)

    except Exception as e:
        print(f"âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        logging.error(f"ä¸»ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        exit(1)


# Cloud Function å…¥å£é»
def backup_erp_data(request):
    """
    Google Cloud Function å…¥å£é»
    HTTP è§¸ç™¼å™¨å‡½æ•¸ï¼Œç”¨æ–¼å®šæ™‚æˆ–æ‰‹å‹•è§¸ç™¼å‚™ä»½

    Args:
        request: HTTP è«‹æ±‚ç‰©ä»¶

    Returns:
        Tuple: (å›æ‡‰å…§å®¹, HTTP ç‹€æ…‹ç¢¼)
    """
    try:
        # è¼‰å…¥è¨­å®š
        config = load_config_from_env()

        # å»ºç«‹å‚™ä»½ç®¡ç†å™¨
        backup_manager = ERPBackupManager(config)

        # åŸ·è¡Œå‚™ä»½
        result = backup_manager.run_backup()

        if result['status'] == 'success':
            return {
                "status": "success",
                "message": "ERP è³‡æ–™å‚™ä»½å®Œæˆ",
                "data": {
                    "records_processed": result.get('records_transformed', 0),
                    "duration_seconds": result.get('duration_seconds', 0)
                }
            }, 200
        elif result['status'] in ['no_data', 'no_valid_data']:
            return {
                "status": "info",
                "message": "æ²’æœ‰æ–°è³‡æ–™éœ€è¦åŒæ­¥",
                "data": result
            }, 200
        else:
            return {
                "status": "error",
                "message": result.get('error', 'å‚™ä»½å¤±æ•—'),
                "data": result
            }, 500

    except Exception as e:
        logging.error(f"Cloud Function åŸ·è¡Œå¤±æ•—: {e}")
        return {
            "status": "error",
            "message": str(e)
        }, 500


# åŸ·è¡Œä¸»ç¨‹å¼ï¼ˆæœ¬åœ°æ¸¬è©¦ä½¿ç”¨ï¼‰
if __name__ == "__main__":
    main()