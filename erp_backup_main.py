# -*- coding: utf-8 -*-
"""
ERP è³‡æ–™å‚™ä»½ä¸»ç¨‹å¼
æ•´åˆ Ragic API è³‡æ–™ç²å–ã€è³‡æ–™è½‰æ›ã€BigQuery ä¸Šå‚³çš„å®Œæ•´æµç¨‹
"""

import os
import json
import logging
from datetime import datetime, timezone, timedelta
from data_transformer import TAIPEI_TZ # For robust timezone handling
import time
import requests
from typing import Dict, Any, Optional, List
from google.cloud import bigquery

# Sheet æ™‚é–“æ¬„ä½é…ç½®
from sheet_time_field_config import (
    get_time_fields_for_sheet,
    get_sheet_id,
    get_all_sheet_codes,
    is_static_sheet
)

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from ragic_client import RagicClient
from data_transformer import create_transformer, DataTransformer
from bigquery_uploader import create_uploader, BigQueryUploader
from email_notifier import send_backup_notification


class ERPBackupManager:
    """ERP è³‡æ–™å‚™ä»½ç®¡ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å‚™ä»½ç®¡ç†å™¨

        Args:
            config: è¨­å®šå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„è¨­å®šåƒæ•¸
        """
        self.config = config
        self.ragic_client: Optional[RagicClient] = None
        self.transformer: Optional[DataTransformer] = None
        self.uploader: Optional[BigQueryUploader] = None

        # è¨­å®šæ—¥èªŒ
        self._setup_logging()

        # é©—è­‰è¨­å®š
        self._validate_config()

        logging.info("ERP å‚™ä»½ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _setup_logging(self):
        """è¨­å®šæ—¥èªŒæ ¼å¼"""
        log_level = self.config.get('log_level', 'INFO').upper()
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

        logging.basicConfig(
            level=getattr(logging, log_level),
            format=log_format,
            handlers=[
                logging.StreamHandler(),
                # å¯ä»¥åŠ å…¥æª”æ¡ˆè™•ç†å™¨
                # logging.FileHandler('erp_backup.log')
            ]
        )

    def _diagnose_egress(self) -> Dict[str, Any]:
        """
        æœ€å°å°å¤–è¨ºæ–·ï¼šgoogle.comã€api.ipify.orgã€Ragic baseã€‚
        å›å‚³å„ç›®æ¨™ä¹‹ç‹€æ…‹ç¢¼/é€¾æ™‚èˆ‡è€—æ™‚ï¼Œä¸¦å¯«å…¥æ—¥èªŒã€‚
        """
        targets = [
            ("google", "https://www.google.com", 5, {}),
            ("ipify", "https://api.ipify.org", 5, {}),
            ("ragic_base", f"https://ap6.ragic.com/{self.config.get('ragic_account','')}", 15, {}),
        ]
        out: Dict[str, Any] = {}
        sess = requests.Session()
        # è‹¥æä¾›é‡‘é‘°ï¼Œé™„ä¸Š headerï¼ˆRagic base ä¸ä¸€å®šéœ€è¦ï¼Œä½†ä¸å½±éŸ¿ï¼‰
        ak = self.config.get('ragic_api_key')
        if ak:
            sess.headers['Authorization'] = f'Basic {ak}'
        for name, url, to, params in targets:
            t0 = time.time()
            try:
                r = sess.get(url, params=params, timeout=to)
                dt = round(time.time() - t0, 3)
                out[name] = {"ok": True, "status": r.status_code, "elapsed_s": dt}
                logging.info(f"egress diag {name}: {url} -> {r.status_code} in {dt}s")
            except Exception as e:
                dt = round(time.time() - t0, 3)
                out[name] = {"ok": False, "error": str(e), "elapsed_s": dt}
                logging.error(f"egress diag {name} error: {e} ({url}) in {dt}s")
        return out

    def _validate_config(self):
        """é©—è­‰è¨­å®šåƒæ•¸"""
        required_fields = [
            'ragic_api_key',
            'ragic_account',
            'ragic_sheet_id',
            'gcp_project_id',
            'bigquery_dataset',
            'bigquery_table'
        ]

        missing_fields = []
        for field in required_fields:
            if not self.config.get(field):
                missing_fields.append(field)

        if missing_fields:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„è¨­å®šæ¬„ä½: {', '.join(missing_fields)}")

        logging.info("è¨­å®šé©—è­‰å®Œæˆ")

    def initialize_clients(self):
        """åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ¶ç«¯"""
        try:
            # åˆå§‹åŒ– Ragic å®¢æˆ¶ç«¯
            self.ragic_client = RagicClient(
                api_key=self.config['ragic_api_key'],
                account=self.config['ragic_account'],
                timeout=self.config.get('ragic_timeout', 30),
                max_retries=self.config.get('ragic_max_retries', 3)
            )

            # åˆå§‹åŒ–è³‡æ–™è½‰æ›å™¨ï¼ˆå–®è¡¨æµç¨‹éœ€å¸¶å…¥æ­£ç¢º sheet_codeï¼Œé¿å…é è¨­ç‚º 99ï¼‰
            sheet_code_cfg = self.config.get('sheet_code')
            if not sheet_code_cfg:
                # å˜—è©¦ç”± sheet_id åæŸ¥ code
                try:
                    smap = self._get_sheet_map()
                    for sc, sid in smap.items():
                        if sid == self.config.get('ragic_sheet_id'):
                            sheet_code_cfg = sc
                            break
                except Exception:
                    pass
            self.transformer = create_transformer(sheet_code=sheet_code_cfg or '99')

            # åˆå§‹åŒ– BigQuery ä¸Šå‚³å™¨
            self.uploader = create_uploader(
                project_id=self.config['gcp_project_id'],
                location=self.config.get('bigquery_location', 'US')
            )

            logging.info("æ‰€æœ‰å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logging.error(f"å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def test_connections(self) -> Dict[str, bool]:
        """
        æ¸¬è©¦æ‰€æœ‰é€£ç·š

        Returns:
            Dict[str, bool]: å„æœå‹™çš„é€£ç·šç‹€æ…‹
        """
        results = {}

        # æ¸¬è©¦ Ragic é€£ç·š
        if self.ragic_client:
            results['ragic'] = self.ragic_client.test_connection()
        else:
            results['ragic'] = False

        # æ¸¬è©¦ BigQuery é€£ç·š
        if self.uploader:
            results['bigquery'] = self.uploader.test_connection()
        else:
            results['bigquery'] = False

        logging.info(f"é€£ç·šæ¸¬è©¦çµæœ: {results}")
        return results

    

    

    def fetch_ragic_data_for_sheet(self, sheet_id: str, last_sync_time: str, last_modified_names: Optional[List[str]] = None, limit: Optional[int] = None, max_pages: Optional[int] = None) -> list:
        """é‡å°æŒ‡å®š sheet ä»¥ã€Œå–®é æŠ“å– + æœ¬åœ°éæ¿¾ä¸€é€±ï¼ˆå¿…è¦æ™‚è‡ªå‹•ç¿»é ï¼‰ã€å–å¾—è³‡æ–™ã€‚"""
        if not self.ragic_client:
            raise Exception("Ragic å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")

        logging.info(f"[fetch_ragic_data_for_sheet] é–‹å§‹è™•ç† {sheet_id}")
        logging.info(f"[fetch_ragic_data_for_sheet] last_sync_time è¼¸å…¥: {last_sync_time.isoformat()}") # Log isoformat

        # last_sync_time å·²ç¶“æ˜¯ UTC timezone-aware datetime ç‰©ä»¶ï¼Œç›´æ¥ä½¿ç”¨
        since_dt = last_sync_time
        logging.info(f"[fetch_ragic_data_for_sheet] ä½¿ç”¨çš„ since_dt (UTC): {since_dt.isoformat()}")

        # å¦‚æœ last_sync_time æ˜¯ None (ä¸æ‡‰è©²ç™¼ç”Ÿï¼Œä½†ä½œç‚ºé˜²ç¦¦æ€§ç·¨ç¨‹)
        if since_dt is None:
            since_dt = datetime.datetime.now(timezone.utc) - datetime.timedelta(days=7)
            logging.warning(f"[fetch_ragic_data_for_sheet] last_sync_time ç‚ºç©ºï¼Œä½¿ç”¨é è¨­ä¸€é€±å‰ (UTC): {since_dt.isoformat()}")

        # æ¬„ä½åç¨±æ¸…å–®ï¼ˆå„ªå…ˆé †åºï¼šåƒæ•¸ > sheet_idé…ç½® > ç’°å¢ƒè®Šæ•¸ > é è¨­å€¼ï¼‰
        if last_modified_names and isinstance(last_modified_names, list) and last_modified_names:
            names = last_modified_names
            logging.info(f"[fetch_ragic_data_for_sheet] ä½¿ç”¨åƒæ•¸æä¾›çš„æ™‚é–“æ¬„ä½: {names}")
        else:
            # å˜—è©¦å¾ sheet_id æ¨æ–· sheet_code ä¸¦ä½¿ç”¨é…ç½®
            from sheet_time_field_config import SHEET_ID_MAP
            sheet_code = None
            for code, sid in SHEET_ID_MAP.items():
                if sid == sheet_id:
                    sheet_code = code
                    break

            if sheet_code:
                names = get_time_fields_for_sheet(sheet_code)
                logging.info(f"[fetch_ragic_data_for_sheet] ä½¿ç”¨ Sheet {sheet_code} é…ç½®çš„æ™‚é–“æ¬„ä½: {names}")
            else:
                # If sheet_code cannot be inferred, get_time_fields_for_sheet will return DEFAULT_TIME_FIELDS
                names = get_time_fields_for_sheet(None) # Pass None or an empty string to trigger default
                logging.warning(f"[fetch_ragic_data_for_sheet] æœªçŸ¥ sheet_id={sheet_id}ï¼Œä½¿ç”¨é è¨­æ™‚é–“æ¬„ä½: {names}")

        # å–®é é™åˆ¶èˆ‡æœ€å¤§é æ•¸
        default_limit = self.config.get('ragic_page_size', 1000)
        per_sheet_boost = {'forms8/17': 3000, 'forms8/2': 3000, 'forms8/3': 3000}
        lim = limit if isinstance(limit, int) and limit > 0 else per_sheet_boost.get(sheet_id, default_limit)
        maxp = int(max_pages) if isinstance(max_pages, int) and max_pages else int(self.config.get('ragic_max_pages', 50))
        logging.info(f"[fetch_ragic_data_for_sheet] åˆ†é è¨­å®š: limit={lim}, max_pages={maxp}")

        # ä¸Šç•Œï¼ˆæ¸¬è©¦ç”¨ï¼‰ï¼šFORCE_UNTIL_ISO / FORCE_UNTIL_DAYS
        until_dt = None
        fui = os.environ.get('FORCE_UNTIL_ISO')
        fud = os.environ.get('FORCE_UNTIL_DAYS')
        if fui:
            try:
                until_dt = datetime.datetime.fromisoformat(fui.replace('Z', '+00:00')).replace(tzinfo=None)
                logging.info(f"[fetch_ragic_data_for_sheet] ä½¿ç”¨ FORCE_UNTIL_ISO: {until_dt.strftime('%Y-%m-%d %H:%M:%S')}")
            except Exception:
                logging.warning(f"FORCE_UNTIL_ISO éæ³•ï¼Œå¿½ç•¥: {fui}")
        elif fud:
            try:
                d = int(fud)
                # ä½¿ç”¨å°åŒ—æ™‚é–“
                until_dt = datetime.datetime.now(timezone.utc) - datetime.timedelta(days=d)
                logging.info(f"[fetch_ragic_data_for_sheet] ä½¿ç”¨ FORCE_UNTIL_DAYSï¼ˆå°åŒ—æ™‚é–“ï¼‰: {until_dt.strftime('%Y-%m-%d %H:%M:%S')}")
            except Exception:
                logging.warning(f"FORCE_UNTIL_DAYS éæ³•ï¼Œå¿½ç•¥: {fud}")

        data = self.ragic_client.fetch_since_local_paged(
            sheet_id=sheet_id,
            since_dt=since_dt,
            last_modified_field_names=names,
            until_dt=until_dt,
            limit=lim,
            max_pages=maxp,
            no_new_data_pages_threshold=self.config.get('ragic_no_new_data_pages_threshold')
        )
        logging.info(f"[fetch_ragic_data_for_sheet] [{sheet_id}] å–å¾— {len(data)} ç­†ï¼ˆlocal paged incrementalï¼‰")
        return data

    def transform_data(self, ragic_data: list) -> list:
        """
        è½‰æ›è³‡æ–™æ ¼å¼

        Args:
            ragic_data: åŸå§‹ Ragic è³‡æ–™

        Returns:
            list: è½‰æ›å¾Œçš„è³‡æ–™

        Raises:
            Exception: ç•¶è³‡æ–™è½‰æ›å¤±æ•—æ™‚
        """
        if not self.transformer:
            raise Exception("è³‡æ–™è½‰æ›å™¨æœªåˆå§‹åŒ–")

        logging.info("é–‹å§‹è³‡æ–™è½‰æ›...")

        try:
            transformed_data = self.transformer.transform_data(ragic_data)
            logging.info(f"æˆåŠŸè½‰æ› {len(transformed_data)} ç­†è³‡æ–™")
            return transformed_data

        except Exception as e:
            logging.error(f"è³‡æ–™è½‰æ›å¤±æ•—: {e}")
            raise

    def upload_to_bigquery(self, transformed_data: list) -> Dict[str, Any]:
        """
        ä¸Šå‚³è³‡æ–™è‡³ BigQuery

        Args:
            transformed_data: è½‰æ›å¾Œçš„è³‡æ–™

        Returns:
            Dict[str, Any]: ä¸Šå‚³çµæœ

        Raises:
            Exception: ç•¶ä¸Šå‚³å¤±æ•—æ™‚
        """
        if not self.uploader:
            raise Exception("BigQuery ä¸Šå‚³å™¨æœªåˆå§‹åŒ–")

        logging.info("é–‹å§‹ä¸Šå‚³è³‡æ–™è‡³ BigQuery...")
        # æ¸¬è©¦æ¨¡å¼ï¼šè‹¥ç’°å¢ƒè®Šæ•¸ TRUNCATE_BEFORE=trueï¼Œå…ˆæ¸…ç©ºç›®æ¨™è¡¨
        try:
            if os.environ.get('TRUNCATE_BEFORE', 'false').lower() == 'true' and self.uploader:
                logging.info("TRUNCATE_BEFORE=trueï¼Œæ¸…ç©ºç›®æ¨™è¡¨ä»¥é¿å…æ¸¬è©¦æ™‚æ™‚é–“å·®é€ æˆæ··æ·†")
                self.uploader.truncate_table(self.config['bigquery_dataset'], self.config['bigquery_table'])
        except Exception as e:
            logging.warning(f"é æ¸…ç©ºç›®æ¨™è¡¨å¤±æ•—ï¼ˆéè‡´å‘½ï¼‰ï¼š{e}")

        try:
            # æ ¹æ“šè³‡æ–™é‡æ±ºå®šæ˜¯å¦æ‰¹æ¬¡ä¸Šå‚³èˆ‡ä¸Šå‚³æ¨¡å¼
            batch_size = self.config.get('upload_batch_size', 1000)
            use_merge = self.config.get('use_merge', True)
            upload_mode = self.config.get('upload_mode', 'auto')
            batch_threshold = self.config.get('batch_threshold', 5000)
            staging_table = self.config.get('staging_table')
            merge_sp_name = self.config.get('merge_sp_name')

            if len(transformed_data) > batch_size:
                from bigquery_uploader import batch_upload_data
                results = batch_upload_data(
                    self.uploader,
                    transformed_data,
                    self.config['bigquery_dataset'],
                    self.config['bigquery_table'],
                    batch_size
                )

                # å½™ç¸½çµæœ
                total_processed = sum(r.get('records_processed', 0) for r in results)
                success_batches = sum(1 for r in results if r.get('status') == 'success')

                return {
                    "status": "success" if success_batches == len(results) else "partial_success",
                    "method": "batch_upload",
                    "total_batches": len(results),
                    "success_batches": success_batches,
                    "total_records_processed": total_processed,
                    "batch_results": results
                }
            else:
                result = self.uploader.upload_data(
                    transformed_data,
                    self.config['bigquery_dataset'],
                    self.config['bigquery_table'],
                    use_merge=use_merge,
                    upload_mode=upload_mode,
                    batch_threshold=batch_threshold,
                    staging_table=staging_table,
                    merge_sp_name=merge_sp_name
                )

                logging.info(f"æˆåŠŸä¸Šå‚³ {result.get('records_processed', 0)} ç­†è³‡æ–™")
                return result

        except Exception as e:
            logging.error(f"BigQuery ä¸Šå‚³å¤±æ•—: {e}")
            raise

    def _write_run_result(self, agg_id: Optional[str], sheet_code: str, result: Dict[str, Any]) -> None:
        try:
            client = bigquery.Client(project=self.config['gcp_project_id'])
            table = f"{self.config['gcp_project_id']}.erp_backup.run_results"
            details = (result.get('details') or [{}])
            d0 = details[0] if isinstance(details, list) and details else {}
            rows = [{
                'agg_id': agg_id or '',
                'sheet_code': sheet_code,
                'status': result.get('status'),
                'uploaded': d0.get('uploaded', result.get('records_processed', 0)),
                'invalid': d0.get('invalid', result.get('invalid_records', 0)),
                'fetched': d0.get('fetched', result.get('records_fetched', 0)),
                'error': result.get('error'),
                'start_time': result.get('start_time'),
                'end_time': result.get('end_time'),
            }]
            errors = client.insert_rows_json(table, rows)
            if errors:
                logging.warning(f"å¯«å…¥ run_results å¤±æ•—: {errors}")
            else:
                logging.info(f"run_results å·²è¨˜éŒ„: agg_id={agg_id}, sheet={sheet_code}")
        except Exception as e:
            logging.warning(f"run_results è¨˜éŒ„å¤±æ•—: {e}")

    def run_backup(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„å‚™ä»½æµç¨‹

        Returns:
            Dict[str, Any]: å‚™ä»½çµæœçµ±è¨ˆ

        Raises:
            Exception: ç•¶å‚™ä»½æµç¨‹å¤±æ•—æ™‚
        """
        start_time = datetime.now(timezone.utc)
        logging.info(f"é–‹å§‹åŸ·è¡Œ ERP è³‡æ–™å‚™ä»½æµç¨‹ - {start_time}")

        # åˆå§‹åŒ– result è®Šæ•¸ï¼Œé¿å… finally å€å¡Šä¸­æ‰¾ä¸åˆ°è®Šæ•¸
        result = {
            "status": "error",
            "start_time": start_time.isoformat(),
            "error": "æœªçŸ¥éŒ¯èª¤"
        }

        try:
            # å…ˆåšå°å¤–è¨ºæ–·ï¼Œå”åŠ©é‡æ¸…é›²ç«¯é€£ç·šç‹€æ³
            diagnostics = self._diagnose_egress()
            # åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ¶ç«¯
            self.initialize_clients()

            # æ¸¬è©¦é€£ç·š
            connections = self.test_connections()
            if not all(connections.values()):
                failed_services = [k for k, v in connections.items() if not v]
                raise Exception(f"æœå‹™é€£ç·šå¤±æ•—: {', '.join(failed_services)}")

            # å–®è¡¨æ¨¡å¼ä¸‹ï¼Œå¾ sheet_sync_state ç²å–æœ€å¾ŒåŒæ­¥æ™‚é–“
            sheet_code_for_sync = self.config.get('sheet_code') or '99' # é è¨­ç‚º 99
            last_sync_time = self.uploader.get_last_sync_timestamp_by_sheet(sheet_code_for_sync)
            logging.info(f"æœ€å¾ŒåŒæ­¥æ™‚é–“: {last_sync_time}")

            # å–®è¡¨æ¨¡å¼ï¼šå¾ Ragic ç²å–è³‡æ–™
            ragic_data = self.fetch_ragic_data_for_sheet(
                sheet_id=self.config['ragic_sheet_id'],
                last_sync_time=last_sync_time,
                last_modified_names=get_time_fields_for_sheet(sheet_code_for_sync)
            )

            if not ragic_data:
                logging.info("Ragic ç„¡æ–°è³‡æ–™éœ€è¦å‚™ä»½")
                end_time = datetime.datetime.now(timezone.utc)
                result = {
                    "status": "no_data",
                    "message": "Ragic ç„¡æ–°è³‡æ–™å¯ä»¥å‚™ä»½",
                    "start_time": start_time.isoformat(),
                    "end_time": end_time.isoformat(),
                    "duration_seconds": (end_time - start_time).total_seconds(),
                    "last_sync_time": last_sync_time,
                    "records_processed": 0
                }
                # è¨˜éŒ„è‡³ run_resultsï¼ˆå³ä½¿ç„¡è³‡æ–™ï¼‰
                try:
                    agg_id = self.config.get('agg_id') or os.environ.get('AGG_ID')
                    # å¾ sheet_map åæŸ¥è¡¨å–®ä»£ç¢¼
                    sheet_id = self.config.get('ragic_sheet_id')
                    sheet_code = None
                    try:
                        smap = self._get_sheet_map()
                        for sc, sid in smap.items():
                            if sid == sheet_id:
                                sheet_code = sc
                                break
                    except Exception:
                        pass
                    if sheet_code:
                        self._write_run_result(agg_id, sheet_code, result)
                except Exception as e:
                    logging.warning(f"è¨˜éŒ„ run_result å¤±æ•—ï¼ˆç„¡è³‡æ–™æƒ…æ³ï¼‰: {e}")
                return result

            # æ­¥é©Ÿ 2: è½‰æ›è³‡æ–™
            transformed_data = self.transform_data(ragic_data)

            if not transformed_data:
                logging.warning("è½‰æ›å¾Œæ²’æœ‰æœ‰æ•ˆè³‡æ–™")
                result = {
                    "status": "no_valid_data",
                    "start_time": start_time.isoformat(),
                    "end_time": datetime.datetime.now(timezone.utc).isoformat(),
                    "duration_seconds": (datetime.datetime.now(timezone.utc) - start_time).total_seconds(),
                    "records_fetched": len(ragic_data),
                    "records_processed": 0
                }
                # è¨˜éŒ„è‡³ run_resultsï¼ˆç„¡æœ‰æ•ˆè³‡æ–™æƒ…æ³ï¼‰
                try:
                    agg_id = self.config.get('agg_id') or os.environ.get('AGG_ID')
                    # å¾ sheet_map åæŸ¥è¡¨å–®ä»£ç¢¼
                    sheet_id = self.config.get('ragic_sheet_id')
                    sheet_code = None
                    try:
                        smap = self._get_sheet_map()
                        for sc, sid in smap.items():
                            if sid == sheet_id:
                                sheet_code = sc
                                break
                    except Exception:
                        pass
                    if sheet_code:
                        self._write_run_result(agg_id, sheet_code, result)
                except Exception as e:
                    logging.warning(f"è¨˜éŒ„ run_result å¤±æ•—ï¼ˆç„¡æœ‰æ•ˆè³‡æ–™æƒ…æ³ï¼‰: {e}")
                return result

            # æ­¥é©Ÿ 3: ä¸Šå‚³è‡³ BigQuery
            upload_result = self.upload_to_bigquery(transformed_data)

            # è¨ˆç®—ç¸½è€—æ™‚
            end_time = datetime.datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()

            # æ•´åˆçµæœï¼ˆå–®è¡¨ï¼‰
            # å–å¾— sheet_code èˆ‡ sheet_name ä»¥ä¾›éƒµä»¶é¡¯ç¤º
            sheet_id = self.config.get('ragic_sheet_id')
            sheet_code_for_mail = self.config.get('sheet_code') or ''
            if not sheet_code_for_mail and sheet_id:
                try:
                    # ç”±å°ç…§è¡¨åæŸ¥ code
                    smap = self._get_sheet_map()
                    for sc, sid in smap.items():
                        if sid == sheet_id:
                            sheet_code_for_mail = sc
                            break
                except Exception:
                    pass
            sheet_name_for_mail = sheet_code_for_mail or (sheet_id or '')

            result = {
                "status": "success",
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "records_fetched": len(ragic_data),
                "records_transformed": len(transformed_data),
                "records_processed": upload_result.get('records_processed', len(transformed_data)),
                "invalid_records": len(self.transformer.get_invalid_records()) if hasattr(self.transformer, 'get_invalid_records') else 0,
                "details": [
                    {
                        "sheet_code": sheet_code_for_mail,
                        "sheet_name": sheet_name_for_mail,
                        "uploaded": upload_result.get('records_processed', len(transformed_data)),
                        "invalid": len(self.transformer.get_invalid_records()) if hasattr(self.transformer, 'get_invalid_records') else 0,
                        "fetched": len(ragic_data)
                    }
                ],
                "upload_result": upload_result,
                "diagnostics": diagnostics
            }
            # è¨˜éŒ„è‡³ run_results
            try:
                agg_id = self.config.get('agg_id') or os.environ.get('AGG_ID')
                if sheet_code_for_mail:
                    self._write_run_result(agg_id, sheet_code_for_mail, result)
            except Exception:
                pass

            logging.info(f"å‚™ä»½æµç¨‹å®Œæˆ - è€—æ™‚: {duration:.2f} ç§’")
            return result

        except Exception as e:
            end_time = datetime.datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            error_msg = f"å‚™ä»½å¤±æ•— - è€—æ™‚: {duration:.2f} ç§’, éŒ¯èª¤: {str(e)}"
            logging.error(error_msg)

            # å°‡éŒ¯èª¤åŸå› ä»¥çµæ§‹åŒ–å½¢å¼å¯«å…¥æ—¥èªŒï¼Œä¾¿æ–¼ Cloud Logging éæ¿¾
            try:
                logging.error({
                    "type": "backup_error",
                    "sheet_id": self.config.get('ragic_sheet_id'),
                    "dataset": self.config.get('bigquery_dataset'),
                    "table": self.config.get('bigquery_table'),
                    "error": str(e),
                    "duration_seconds": duration,
                    "start_time": start_time.isoformat(),
                })
            except Exception:
                pass

            result = {
                "status": "error",
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "error": str(e),
                "diagnostics": locals().get('diagnostics')
            }
            return result

        finally:
            # æ¸…ç†è³‡æº
            self.cleanup()
            # å–®è¡¨æµç¨‹ï¼šæ­¤è™•éƒµä»¶ä¿ç•™ï¼ˆå…¼å®¹èˆŠæµç¨‹ï¼‰
            try:
                if self._should_send_notification():
                    self._send_email_notification(result)
            except Exception as e:
                logging.warning(f"ç™¼é€é›»å­éƒµä»¶é€šçŸ¥å¤±æ•—: {e}")

    def _should_send_notification(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²ç™¼é€é›»å­éƒµä»¶é€šçŸ¥"""
        if os.environ.get('DISABLE_EMAIL', 'false').lower() == 'true':
            return False
        return (
            self.config.get('notification_email') and
            self.config.get('smtp_from_email') and
            self.config.get('smtp_from_password')
        )

    def _send_email_notification(self, backup_result: Dict[str, Any]):
        """ç™¼é€é›»å­éƒµä»¶é€šçŸ¥"""
        smtp_config = {
            'from_email': self.config.get('smtp_from_email'),
            'from_password': self.config.get('smtp_from_password'),
            'smtp_server': self.config.get('smtp_server', 'smtp.gmail.com'),
            'smtp_port': self.config.get('smtp_port', 587)
        }

        success = send_backup_notification(
            project_id=self.config['gcp_project_id'],
            to_email=self.config['notification_email'],
            backup_result=backup_result,
            smtp_config=smtp_config
        )

        if success:
            logging.info(f"é›»å­éƒµä»¶é€šçŸ¥å·²ç™¼é€è‡³: {self.config['notification_email']}")
        else:
            logging.error("é›»å­éƒµä»¶é€šçŸ¥ç™¼é€å¤±æ•—")

    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        try:
            if self.ragic_client:
                self.ragic_client.close()
            if self.uploader:
                self.uploader.close()
            logging.info("è³‡æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logging.warning(f"è³‡æºæ¸…ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    # ---- sheet å°ç…§ ----
    def _get_sheet_map(self) -> Dict[str, str]:
        """å–å¾— sheet_code â†’ sheet_id å°ç…§è¡¨ï¼Œå„ªå…ˆä½¿ç”¨ç’°å¢ƒè¨­å®šï¼Œå…¶æ¬¡å…§å»ºé è¨­ã€‚"""
        # å…§å»ºé è¨­
        default_map = {
            '10': 'forms8/5',
            '20': 'forms8/4',
            '30': 'forms8/7',
            '40': 'forms8/1',
            '41': 'forms8/6',
            '50': 'forms8/17',
            '60': 'forms8/2',
            '70': 'forms8/9',
            '99': 'forms8/3',
        }
        # 1) è¨­å®šä¸­å·²æœ‰è§£æå®Œæˆçš„ dict
        m = self.config.get('sheet_map')
        if isinstance(m, dict) and m:
            return m
        # 2) JSON å­—ä¸²
        mjson = self.config.get('sheet_map_json')
        if mjson:
            try:
                parsed = json.loads(mjson)
                if isinstance(parsed, dict) and parsed:
                    return parsed
            except Exception as e:
                logging.warning(f"SHEET_MAP_JSON è§£æå¤±æ•—ï¼Œä½¿ç”¨é è¨­å°ç…§: {e}")
        # 3) æª”æ¡ˆ
        mfile = self.config.get('sheet_map_file')
        if mfile and os.path.exists(mfile):
            try:
                with open(mfile, 'r', encoding='utf-8') as f:
                    parsed = json.load(f)
                    if isinstance(parsed, dict) and parsed:
                        return parsed
            except Exception as e:
                logging.warning(f"SHEET_MAP_FILE è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­å°ç…§: {e}")
        return default_map

    def run_backup_all_sheets(self) -> Dict[str, Any]:
        """å¤šè¡¨æµç¨‹ï¼šä¾å›ºå®š 9 å¼µè¡¨ï¼ˆæˆ–ç’°å¢ƒè®Šæ•¸æä¾›ï¼‰é€²è¡Œä¸€é€±å¢é‡æŠ“å–ã€è½‰æ›ã€ä¸Šå‚³ä¸¦å½™ç¸½ã€‚"""
        start_time = datetime.now(timezone.utc)
        logging.info(f"é–‹å§‹åŸ·è¡Œ ERP å¤šè¡¨å‚™ä»½æµç¨‹ - {start_time}")

        result = {
            "status": "error",
            "start_time": start_time.isoformat(),
            "error": "æœªçŸ¥éŒ¯èª¤"
        }

        # 9 å¼µè¡¨å°ç…§ï¼ˆå¯ç”±å¤–éƒ¨è¨­å®šè¦†è“‹ï¼‰
        sheets = self._get_sheet_map()

        # é€£ç·šèˆ‡æœ€å¾ŒåŒæ­¥æ™‚é–“
        try:
            self.initialize_clients()
            connections = self.test_connections()
            if not all(connections.values()):
                failed_services = [k for k, v in connections.items() if not v]
                raise Exception(f"æœå‹™é€£ç·šå¤±æ•—: {', '.join(failed_services)}")

            

            total_uploaded = 0
            total_invalid = 0
            details: List[Dict[str, Any]] = []
            all_fetched_records: Dict[str, List[Dict[str, Any]]] = {} # å„²å­˜åŸå§‹æŠ“å–åˆ°çš„è¨˜éŒ„

            # é€è¡¨è™•ç†
            for sheet_code, sheet_id in sheets.items():
                try:
                    # è¦å‰‡ï¼šè‹¥è¨­ FORCE_SINCE_DAYS/ISO å‰‡è¦†è“‹ per-sheetï¼›å¦å‰‡ä½¿ç”¨ per-sheet last syncã€‚
                    force_iso = os.environ.get('FORCE_SINCE_ISO')
                    force_days = os.environ.get('FORCE_SINCE_DAYS')
                    ls: datetime.datetime # ç¢ºä¿ ls æ˜¯ datetime ç‰©ä»¶

                    if force_iso:
                        ls = datetime.datetime.fromisoformat(force_iso.replace('Z', '+00:00')).astimezone(timezone.utc)
                    elif force_days:
                        days = int(force_days)
                        ls = datetime.datetime.now(timezone.utc) - datetime.timedelta(days=days)
                    else:
                        # å¾ sheet_sync_state ç²å–æœ€å¾ŒåŒæ­¥æ™‚é–“
                        ls = self.uploader.get_last_sync_timestamp_by_sheet(sheet_code)

                    logging.info(f"[Sheet {sheet_code}] ä½¿ç”¨çš„æœ€å¾ŒåŒæ­¥æ™‚é–“ (UTC): {ls.isoformat()}")

                    # ç²å–è©²è¡¨å–®å°ˆå±¬çš„æ™‚é–“æ¬„ä½æ¸…å–®
                    last_modified_names_for_sheet = get_time_fields_for_sheet(sheet_code)
                    logging.info(f"[Sheet {sheet_code}] ä½¿ç”¨çš„æ™‚é–“æ¬„ä½: {last_modified_names_for_sheet}")

                    # åˆ‡æ›ï¼šè‹¥ USE_RAGIC_WHERE=trueï¼Œæ”¹ç”¨ä¼ºæœç«¯ whereï¼ˆé¿å…æ’åºå½±éŸ¿ï¼‰
                    use_where = os.environ.get('USE_RAGIC_WHERE', 'false').lower() == 'true'
                    if use_where:
                        # ç›´æ¥ç”¨ fetch_dataï¼ˆwhereï¼‰ï¼Œé™åˆ¶ 1 é 
                        per_limit = self.config.get('ragic_page_size', 1000)
                        # å…è¨±æä¾›æ¯è¡¨ where æ¬„ä½ï¼ˆæ¬„ä½ ID æˆ–ç³»çµ±éµï¼‰ï¼›é è¨­ _ragicModified
                        per_sheet_where = {
                            'forms8/3': os.environ.get('RAGIC_WHERE_FIELD_99'),
                        }
                        wfield = per_sheet_where.get(sheet_id) or os.environ.get('RAGIC_WHERE_FIELD')
                        # fetch_data æœŸæœ› Ragic æ ¼å¼å­—ä¸²
                        records = self.ragic_client.fetch_data(sheet_id, last_sync_time=ls.strftime('%Y/%m/%d %H:%M:%S'), limit=per_limit, max_pages=1, where_field=wfield)
                    else:
                        records = self.fetch_ragic_data_for_sheet(
                            sheet_id=sheet_id,
                            last_sync_time=ls,
                            last_modified_names=last_modified_names_for_sheet,
                            limit=None,
                            max_pages=None
                        )

                    # å¦‚æœæ˜¯æ¸¬è©¦æ¨¡å¼ï¼ŒåªæŠ“å–è³‡æ–™ä¸¦è¿”å›
                    if self.config.get('test_fetch_only'):
                        details.append({
                            'sheet_code': sheet_code,
                            'sheet_name': sheet_code,
                            'fetched': len(records),
                            'last_sync_used': ls.isoformat(),
                            'records_sample': records[:5] # åªå–å‰5ç­†ä½œç‚ºæ¨£æœ¬
                        })
                        all_fetched_records[sheet_code] = records
                        continue # è·³éè½‰æ›å’Œä¸Šå‚³

                    # è‹¥ä»ç‚º 0ï¼šåœ¨å•Ÿç”¨ skip_if_no_recent_days æ™‚ç›´æ¥è·³éï¼›å¦å‰‡ä¿ç•™åŸå…ˆç…™éœ§æ¸¬è©¦
                    if not records:
                        # ç²¾æº–é‚è¼¯ï¼šç„¡æ–°è³‡æ–™å³è·³éï¼Œä¸åšç…™éœ§æ¸¬è©¦
                        logging.info(f"{sheet_code} ç„¡æ–°è³‡æ–™ï¼Œè·³éä¸Šå‚³ï¼ˆlast_sync_used={ls.isoformat()})")
                        details.append({
                            'sheet_code': sheet_code,
                            'sheet_name': sheet_code,
                            'uploaded': 0,
                            'invalid': 0,
                            'fetched': 0,
                            'last_sync_used': ls.isoformat(),
                            'skipped': True,
                            'reason': 'no_new_data'
                        })
                        continue

                    transformer = create_transformer(sheet_code=sheet_code, project_id=self.config['gcp_project_id'], use_dynamic_mapping=False)
                    transformed = transformer.transform_data(records)

                    uploaded = 0
                    invalid = len(transformer.get_invalid_records()) if hasattr(transformer, 'get_invalid_records') else 0
                    if transformed:
                        up_res = self.uploader.upload_data(
                            data=transformed,
                            dataset_id=self.config['bigquery_dataset'],
                            table_id=self.config['bigquery_table'],
                            use_merge=self.config.get('use_merge', False),
                            upload_mode=self.config.get('upload_mode', 'direct')
                        )
                        uploaded = up_res.get('records_processed', len(transformed))

                    # æˆåŠŸä¸Šå‚³å¾Œï¼Œæ›´æ–° sheet_sync_state
                        if up_res.get('status') == 'success':
                            # æ‰¾å‡ºé€™æ‰¹è³‡æ–™ä¸­æœ€æ–°çš„æ™‚é–“æˆ³
                            latest_timestamp_in_batch = None
                            for record in transformed:
                                for field_name in last_modified_names_for_sheet:
                                    if field_name in record and record[field_name]:
                                        dt = self.ragic_client._parse_dt(record[field_name])
                                        if dt and (latest_timestamp_in_batch is None or dt > latest_timestamp_in_batch):
                                            latest_timestamp_in_batch = dt
                                            break # æ‰¾åˆ°ä¸€å€‹æœ‰æ•ˆæ™‚é–“å°±è·³å‡ºå…§å±¤å¾ªç’°

                            if latest_timestamp_in_batch:
                                self.uploader.update_sync_timestamp(sheet_code, latest_timestamp_in_batch)
                            else:
                                logging.warning(f"æœªèƒ½åœ¨ä¸Šå‚³è³‡æ–™ä¸­æ‰¾åˆ°æœ‰æ•ˆæ™‚é–“æˆ³ä¾†æ›´æ–° sheet_sync_state ({sheet_code})")

                except Exception as se:
                    logging.error(f"è™•ç†è¡¨ {sheet_code} å¤±æ•—: {se}")
                    details.append({
                        'sheet_code': sheet_code,
                        'sheet_name': sheet_code,
                        'uploaded': 0,
                        'invalid': 0,
                        'fetched': 0,
                        'last_sync_used': ls.isoformat() if 'ls' in locals() else None,
                        'error': str(se)
                    })

            end_time = datetime.datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()

            status = 'success' if total_uploaded > 0 else 'no_data'
            result = {
                'status': status,
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration,
                'records_processed': total_uploaded,
                'invalid_records': total_invalid,
                'details': details
            }
            logging.info(f"å¤šè¡¨å‚™ä»½å®Œæˆï¼Œå…±ä¸Šå‚³ {total_uploaded} ç­†ï¼Œç„¡æ•ˆ {total_invalid} ç­†")

            if self.config.get('test_fetch_only'):
                return {'result': result, 'fetched_records': all_fetched_records}
            else:
                return result

        except Exception as e:
            end_time = datetime.datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            logging.error(f"å¤šè¡¨å‚™ä»½å¤±æ•—: {e}")
            return {
                'status': 'error',
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration,
                'error': str(e)
            }
        finally:
            self.cleanup()
            # åƒ…åœ¨å¤šè¡¨æµç¨‹å®Œæˆå¾Œä¸€æ¬¡æ€§å¯„é€é€šçŸ¥
            try:
                if self._should_send_notification():
                    self._send_email_notification(result)
            except Exception as e:
                logging.warning(f"ç™¼é€é›»å­éƒµä»¶é€šçŸ¥å¤±æ•—: {e}")


def load_config_from_env() -> Dict[str, Any]:
    """
    å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥è¨­å®š

    Returns:
        Dict[str, Any]: è¨­å®šå­—å…¸

    Raises:
        ValueError: ç•¶å¿…è¦çš„ç’°å¢ƒè®Šæ•¸ç¼ºå¤±æ™‚
    """
    config = {
        'ragic_api_key': os.environ.get('RAGIC_API_KEY'),
        'ragic_account': os.environ.get('RAGIC_ACCOUNT', 'your-account'),
        'ragic_sheet_id': os.environ.get('RAGIC_SHEET_ID', 'your-sheet/1'),
        'ragic_timeout': int(os.environ.get('RAGIC_TIMEOUT', 30)),
        'ragic_max_retries': int(os.environ.get('RAGIC_MAX_RETRIES', 3)),
        'ragic_page_size': int(os.environ.get('RAGIC_PAGE_SIZE', 1000)),
        'ragic_max_pages': int(os.environ.get('RAGIC_MAX_PAGES', 50)),
        'ragic_no_new_data_pages_threshold': int(os.environ.get('RAGIC_NO_NEW_DATA_PAGES_THRESHOLD', 2)),
        'last_modified_field_names': os.environ.get('LAST_MODIFIED_FIELD_NAMES'),
        # sheet å°ç…§è¨­å®š
        'sheet_map_json': os.environ.get('SHEET_MAP_JSON'),
        'sheet_map_file': os.environ.get('SHEET_MAP_FILE'),

        'gcp_project_id': os.environ.get('GCP_PROJECT_ID', 'your-project-id'),
        'bigquery_dataset': os.environ.get('BIGQUERY_DATASET', 'your_dataset'),
        'bigquery_table': os.environ.get('BIGQUERY_TABLE', 'erp_backup'),
        'bigquery_location': os.environ.get('BIGQUERY_LOCATION', 'US'),

        'upload_batch_size': int(os.environ.get('UPLOAD_BATCH_SIZE', 1000)),
        'use_merge': os.environ.get('USE_MERGE', 'true').lower() == 'true',
        'upload_mode': os.environ.get('UPLOAD_MODE', 'auto'),
        'batch_threshold': int(os.environ.get('BATCH_THRESHOLD', 5000)),
        'staging_table': os.environ.get('STAGING_TABLE'),
        'merge_sp_name': os.environ.get('MERGE_SP_NAME'),
        'log_level': os.environ.get('LOG_LEVEL', 'INFO'),
        # è¿‘ N å¤©ç„¡æ–°å¢å‰‡è·³éï¼ˆé è¨­ 7ï¼‰
        'skip_if_no_recent_days': int(os.environ.get('SKIP_IF_NO_RECENT_DAYS', 7)),
        
        # é›»å­éƒµä»¶é€šçŸ¥è¨­å®š
        'notification_email': os.environ.get('NOTIFICATION_EMAIL'),
        'smtp_from_email': os.environ.get('SMTP_FROM_EMAIL'),
        'smtp_from_password': os.environ.get('SMTP_FROM_PASSWORD'),
        'smtp_server': os.environ.get('SMTP_SERVER', 'smtp.gmail.com'),
        'smtp_port': int(os.environ.get('SMTP_PORT', 587)),
        'test_fetch_only': os.environ.get('TEST_FETCH_ONLY', 'false').lower() == 'true' # New flag
    }

    return config


def main():
    """
    ä¸»ç¨‹å¼å…¥å£é»
    """
    try:
        # è¼‰å…¥è¨­å®š
        config = load_config_from_env()

        # å»ºç«‹å‚™ä»½ç®¡ç†å™¨
        backup_manager = ERPBackupManager(config)

        # è‹¥ RAGIC_SHEET_ID è¨­ç‚º ALLï¼Œèµ°å¤šè¡¨æµç¨‹ï¼›å¦å‰‡èµ°å–®è¡¨
        if (config.get('ragic_sheet_id') or '').upper() == 'ALL':
            if config.get('test_fetch_only'):
                test_result = backup_manager.run_backup_all_sheets()
                result = test_result['result']
                fetched_records = test_result['fetched_records']
                print("\n--- æ¸¬è©¦æ¨¡å¼ï¼šå·²æŠ“å–è³‡æ–™ ---")
                for sheet_code, records in fetched_records.items():
                    print(f"è¡¨å–® {sheet_code} æŠ“å–åˆ° {len(records)} ç­†è³‡æ–™ã€‚")
                    if records:
                        print(f"  å‰ 3 ç­†è³‡æ–™ç¯„ä¾‹: {json.dumps(records[:3], indent=2, ensure_ascii=False)}")
                print("---------------------------\n")
            else:
                result = backup_manager.run_backup_all_sheets()
        else:
            # å…è¨±ç‚ºå–®è¡¨æŒ‡å®š sheet_code ä»¥åˆ© email details é¡¯ç¤º
            try:
                config['sheet_code'] = os.environ.get('RAGIC_SHEET_CODE')
                # è‹¥æä¾› sheet_code ä¸”æœªæä¾›æ˜ç¢º sheet_idï¼Œå˜—è©¦ç”¨å°ç…§è¡¨æ˜ å°„
                sc = config.get('sheet_code')
                sid = config.get('ragic_sheet_id')
                if sc and (not sid or sid == 'AUTO' or sid == 'your-sheet/1'):
                    # è‡¨æ™‚å»ºç«‹ç®¡ç†å™¨ä»¥è®€å–å°ç…§
                    sm = backup_manager._get_sheet_map()
                    mapped = sm.get(sc)
                    if mapped:
                        config['ragic_sheet_id'] = mapped
                        # åŒæ­¥åˆ°å¯¦ä¾‹è¨­å®š
                        backup_manager.config['ragic_sheet_id'] = mapped
                        logging.info(f"ä»¥å°ç…§è¡¨å°‡ sheet_code={sc} æ˜ å°„ç‚º sheet_id={mapped}")
            except Exception:
                pass
            result = backup_manager.run_backup()

        # è¼¸å‡ºçµæœ
        if result['status'] == 'success':
            print(f"âœ… å‚™ä»½æˆåŠŸå®Œæˆ")
            print(f"ğŸ“Š ç²å–è¨˜éŒ„: {result.get('records_fetched', 0)}")
            print(f"ğŸ”„ è½‰æ›è¨˜éŒ„: {result.get('records_transformed', 0)}")
            print(f"â±ï¸  ç¸½è€—æ™‚: {result.get('duration_seconds', 0):.2f} ç§’")
        elif result['status'] in ['no_data', 'no_valid_data']:
            print(f"â„¹ï¸  {result['status']}: æ²’æœ‰éœ€è¦åŒæ­¥çš„è³‡æ–™")
        else:
            print(f"âŒ å‚™ä»½å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            exit(1)

    except Exception as e:
        print(f"âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        logging.error(f"ä¸»ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        exit(1)


# Cloud Function å…¥å£é»
def backup_erp_data(request):
    """
    Google Cloud Function å…¥å£é»
    HTTP è§¸ç™¼å™¨å‡½æ•¸ï¼Œç”¨æ–¼å®šæ™‚æˆ–æ‰‹å‹•è§¸ç™¼å‚™ä»½

    Args:
        request: HTTP è«‹æ±‚ç‰©ä»¶

    Returns:
        Tuple: (å›æ‡‰å…§å®¹, HTTP ç‹€æ…‹ç¢¼)
    """
    try:
        # è§£æè«‹æ±‚ payloadï¼ˆå…è¨±è¦†è“‹éƒ¨åˆ†è¡Œç‚ºï¼‰
        try:
            payload = request.get_json(silent=True) or {}
        except Exception:
            payload = {}
        mode = (payload.get('MODE') if isinstance(payload, dict) else None) or os.environ.get('MODE')
        agg_id_from_req = (payload.get('AGG_ID') if isinstance(payload, dict) else None) or os.environ.get('AGG_ID')
        # å–®æ¬¡åŸ·è¡Œå¯è¦†è“‹æ˜¯å¦å¯„ä¿¡ï¼ˆé¿å…å–®è¡¨æµç¨‹å„è‡ªå¯„ä¿¡ï¼‰
        if isinstance(payload, dict) and 'DISABLE_EMAIL' in payload:
            os.environ['DISABLE_EMAIL'] = str(payload.get('DISABLE_EMAIL')).lower()
        # å…è¨±ä»¥è«‹æ±‚è¦†è“‹å¢é‡æ¬„ä½/where ç­–ç•¥ï¼ˆé¿å…é‡æ–°ä½ˆç½²ï¼‰
        if isinstance(payload, dict):
            if 'USE_RAGIC_WHERE' in payload:
                os.environ['USE_RAGIC_WHERE'] = str(payload.get('USE_RAGIC_WHERE')).lower()
            if 'LAST_MODIFIED_FIELD_NAMES' in payload and isinstance(payload.get('LAST_MODIFIED_FIELD_NAMES'), str):
                os.environ['LAST_MODIFIED_FIELD_NAMES'] = payload.get('LAST_MODIFIED_FIELD_NAMES')
            # é‡è¦ï¼šå°‡è«‹æ±‚çš„ AGG_ID å¯«å…¥ç’°å¢ƒï¼Œè®“å–®è¡¨æµç¨‹èƒ½å¯«å…¥ run_results
            if 'AGG_ID' in payload and isinstance(payload.get('AGG_ID'), str):
                os.environ['AGG_ID'] = payload.get('AGG_ID')

        # è‹¥ç‚ºå½™ç¸½æ¨¡å¼ï¼Œåƒ…å½™ç¸½å¯„ä¿¡ï¼Œä¸åŸ·è¡Œå‚™ä»½
        if mode and mode.upper() == 'AGGREGATE':
            try:
                cfg = load_config_from_env()
                bq = bigquery.Client(project=cfg['gcp_project_id'])
                q = f"""
                SELECT sheet_code, status, uploaded, invalid, fetched, created_at
                FROM `{cfg['gcp_project_id']}.erp_backup.run_results`
                WHERE agg_id = @agg
                  AND created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
                ORDER BY sheet_code
                """
                job = bq.query(q, job_config=bigquery.QueryJobConfig(query_parameters=[bigquery.ScalarQueryParameter('agg', 'STRING', agg_id_from_req or '')]))
                rows = list(job.result())
                details = [{
                    'sheet_code': r['sheet_code'],
                    'sheet_name': r['sheet_code'],
                    'uploaded': r['uploaded'],
                    'invalid': r['invalid'],
                    'fetched': r['fetched'],
                } for r in rows]
                agg_result = {
                    'status': 'success' if any((d.get('uploaded') or 0) > 0 for d in details) else 'no_data',
                    'records_processed': sum(d.get('uploaded') or 0 for d in details),
                    'invalid_records': sum(d.get('invalid') or 0 for d in details),
                    'details': details,
                }
                smtp_config = {
                    'from_email': cfg.get('smtp_from_email'),
                    'from_password': cfg.get('smtp_from_password'),
                    'smtp_server': cfg.get('smtp_server', 'smtp.gmail.com'),
                    'smtp_port': int(cfg.get('smtp_port', 587))
                }
                send_backup_notification(cfg['gcp_project_id'], cfg['notification_email'], agg_result, smtp_config)
                return { 'status': 'success', 'message': 'aggregated and mailed', 'data': agg_result }, 200
            except Exception as e:
                return { 'status': 'error', 'message': str(e) }, 500

        # è¼‰å…¥è¨­å®š
        config = load_config_from_env()

        # å…è¨±ä»¥è«‹æ±‚è¦†å¯«å–®æ¬¡ sheetï¼ˆé¸å¡«ï¼‰
        try:
            if isinstance(payload, dict):
                override_sheet = payload.get('sheet')
                if isinstance(override_sheet, str) and override_sheet:
                    if override_sheet.upper() == 'ALL':
                        config['ragic_sheet_id'] = 'ALL'
                    else:
                        config['ragic_sheet_id'] = override_sheet
        except Exception:
            pass

        # å»ºç«‹å‚™ä»½ç®¡ç†å™¨
        backup_manager = ERPBackupManager(config)

        # ä¾è¨­å®šé¸æ“‡å–®è¡¨æˆ–å¤šè¡¨
        if (config.get('ragic_sheet_id') or '').upper() == 'ALL':
            result = backup_manager.run_backup_all_sheets()
        else:
            result = backup_manager.run_backup()

        if result['status'] == 'success':
            return {
                "status": "success",
                "message": "ERP è³‡æ–™å‚™ä»½å®Œæˆ",
                "data": {
                    "records_processed": result.get('records_transformed', 0) or result.get('records_processed', 0),
                    "duration_seconds": result.get('duration_seconds', 0),
                    "details": result.get('details')
                }
            }, 200
        elif result['status'] in ['no_data', 'no_valid_data']:
            return {
                "status": "info",
                "message": "æ²’æœ‰æ–°è³‡æ–™éœ€è¦åŒæ­¥",
                "data": result
            }, 200
        else:
            return {
                "status": "error",
                "message": result.get('error', 'å‚™ä»½å¤±æ•—'),
                "data": result
            }, 500

    except Exception as e:
        logging.error(f"Cloud Function åŸ·è¡Œå¤±æ•—: {e}")
        return {
            "status": "error",
            "message": str(e)
        }, 500


# åŸ·è¡Œä¸»ç¨‹å¼ï¼ˆæœ¬åœ°æ¸¬è©¦ä½¿ç”¨ï¼‰
if __name__ == "__main__":
    main()